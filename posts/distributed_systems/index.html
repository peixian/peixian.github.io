<!DOCTYPE html>
<html><title>distributed systems</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.4b9309a2ca0781957619d4e9500f6539479ebf8cdcbfaac702d6364b30e2c87c.css"/>
<script defer src="/en.search.min.df03620cdff1eacf15217477a417f6207a4a9fd0d075fe58934a9e03209001fb.js" integrity="sha256-3wNiDN/x6s8VIXR3pBf2IHpKn9DQdf5Yk0qeAyCQAfs="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>distributed systems</h1>

          <p>Tags: <a href="/posts/computers/">computers</a></p>
<ul>
<li><a href="http://muratbuffalo.blogspot.com/2021/02/foundational-distributed-systems-papers.html">http://muratbuffalo.blogspot.com/2021/02/foundational-distributed-systems-papers.html</a></li>
<li><a href="https://github.com/stevana/armstrong-distributed-systems/tree/main">https://github.com/stevana/armstrong-distributed-systems/tree/main</a></li>
</ul>
<h2 id="short-introduction">Short Introduction</h2>
<ul>
<li><a href="http://book.mixu.net/distsys/single-page.html">http://book.mixu.net/distsys/single-page.html</a></li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628</a></li>
</ul>
<h2 id="system-design">System Design</h2>
<h3 id="high-level-methodology">High Level Methodology</h3>
<ul>
<li><a href="https://static.googleusercontent.com/media/sre.google/en//static/pdf/nalsd-workbook-letter.pdf">https://static.googleusercontent.com/media/sre.google/en//static/pdf/nalsd-workbook-letter.pdf</a></li>
<li>Do one thing and do it well
<ul>
<li>Services should be split up into ones that do one thing, makes performance analysis easier and easier for the system to evolve over time</li>
</ul>
</li>
<li>Prefer simple, stateless services where possible
<ul>
<li>State must be kept, but try to reduce the surface area of the state. Stateless components are easy to manage because they&rsquo;re trivial to scale horizontally</li>
</ul>
</li>
<li>Assumptions, constraints, and SLO&rsquo;s
<ul>
<li>Before you start designing, make sure to look at the constraints and non-functional requirements for your system. Have they been state? Are they comprehensive?</li>
<li>At minimum you should understand
<ul>
<li>What the system is supposed to do (business logic)</li>
<li>What the SLOs are around performance, error rates/availability, data loss? If no SLOs, you made need to define them with business owners</li>
<li>Where should it run? What is the budget or what hardware is available?</li>
<li>Expected workload (this may be difficult to gauge)</li>
</ul>
</li>
</ul>
</li>
<li>Getting to an initial high level design (or analysing an existing one)
<ul>
<li>API
<ul>
<li>You need to figure out the API for the system. What operations are available externally? What data goes in, and what results comes out? Do operatinos change the system state, or is it read-only?</li>
<li>Each set of closely related operations is likely to be a service in the system. Many systems have a stateless API layer that coordinates with other parts of the system to serve results</li>
</ul>
</li>
<li>Follow the data
<ul>
<li>After the API, figure out the data flow. What state does the system need to read and/or write in order to do the work? What do we know about the state?</li>
</ul>
</li>
</ul>
</li>
<li>Data consistency - critical data
<ul>
<li>Some applications require very consistent views of critical state</li>
<li>This should be tackled with distributed consensus algorithms to manage state changes, 3/5/+ replicas in different failure domains</li>
<li>Fastpaxos uses a stable leader process to improve performance. Performance for FastPaxos is one rtt between the leader and qurom of closest replicas to commit and update or a consistent read. The client must communicate with the leader, so the percieved latency for the client is the RTT between client/leader + leader/nearest quorum. Stale reads require a roundtrip to the nearest healthy replica, but the data may not be consistent. Other transactions can be batched</li>
</ul>
</li>
<li>Data consistency - less critical data
<ul>
<li>Not all data needs to highly consistent, in these cases replication + backups is often enough.</li>
</ul>
</li>
<li>Data storage considerations
<ul>
<li>Lots of things to think about when storing data, most fundalmental is where are you going to organize the data in RAM and disk</li>
<li>Think about how data is read and written. Which is the most common use case? Usually it&rsquo;s reading.</li>
<li>Think about storing data to match the most common use case. If you&rsquo;re reading in a chronological order by user, consider storing it like that</li>
</ul>
</li>
<li>Sharding data
<ul>
<li>Data obviously needs to be sharded, but watch out for hot shards</li>
<li>Easiest way to shard is m shardes on n servers, where m &gt; 100x n</li>
<li>shard = hash(key) modulko m</li>
<li>we have a map of shard -&gt; server that we look up to find the server</li>
<li>the system can dynamically move shard and update the map in order to rebalance, since shards are relatively small</li>
<li>sharded and replicated datastores should have automatic mechanisms that cross-check state with other replicas and load lost state from peers, useful after downtime and when new replicas are added. Note that this should also be rate-limited somehow to avoid thundering herd problems where many replicas are out of sync</li>
</ul>
</li>
<li>Scaling and Performance
<ul>
<li>Make sure to do some back of the envelope calculations to estimate the potential performance of the system, any scaling limits created by the bottlenecks in the system, and how many machines are likely to be needed to service the given workload</li>
<li>Calculations to use
<ul>
<li>Disk - sanity check the volume of data read/written, disk seeks over time</li>
<li>RAM - how much can we cache? Are we storing an index of some kind?</li>
<li>Bandwidth - size of requests inwards and outwards, do we have enough bandwidth?
<ul>
<li>Bandwidth between datacenters and bandwidth between machines in the same DC</li>
</ul>
</li>
<li>CPU - is this a computationally intensive service? - this can be hard to gauge</li>
<li>What are the estimated concurrent transactions for a service (compute based on throughput per second and latency, e.g. 500 requests per second, 200ms average latency -&gt; 500/(1000/200) -&gt; 100 transactions in flight) - is this a reasonable number? Each request needs some RAM to manage state and some CPU to process</li>
</ul>
</li>
</ul>
</li>
<li>Iterating and refining
<ul>
<li>After a complete design, time to take another look. You might&rsquo;ve unearthed new constraints</li>
<li>Consider
<ul>
<li>Does it work correctly?</li>
<li>Are you meeting your SLOs and constraints?</li>
<li>Is it as simple as it could be?</li>
<li>Is it reliable enough?</li>
<li>Is it making efficient use of hardware?</li>
<li>What assumptions did you make and are there risks?</li>
</ul>
</li>
<li>Are there places you can improve caching, indexing, lookup filters like Bloom filters? Can you degrade gracefully when overloaded by doing cheaper operations?</li>
</ul>
</li>
<li>Monitoring
<ul>
<li>Montiring should be included in the design phase of any distributed system</li>
<li>It needs to
<ul>
<li>Detect outages (and alert)</li>
<li>Provide information on whether you&rsquo;re meeting your SLOs</li>
<li>Give indications of growth and system capacity</li>
<li>Assist in troubleshooting outages and less critical problems</li>
</ul>
</li>
<li>Design metrics for your system and indicate how you will collect data. What are your alert thresholds/conditions?</li>
</ul>
</li>
<li>What could go wrong?
<ul>
<li>Answer these questions
<ul>
<li>What is the scariest thing that can happen to your system?</li>
<li>How does your system defend against it?</li>
<li>Will it continue to meet its SLO? How can you recover, if not?</li>
<li>Can improvements be made to the system to prevent the problem, and what would they cost?</li>
</ul>
</li>
<li>Consider
<ul>
<li>Loss of a data center</li>
<li>Loss of important state/data</li>
<li>Bugs in critical software component</li>
<li>Load spikes</li>
<li>Queries/requests of death that can crash the process</li>
<li>10x growth in workload - will it scale?</li>
<li>Change in data access patterns changes cache hit rate</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="pub-sub">Pub Sub</h3>
<h2 id="theory">Theory</h2>
<ul>
<li><a href="https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/">https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/</a></li>
</ul>
<h3 id="verification">Verification</h3>
<ul>
<li><a href="https://lawrencecpaulson.github.io/2022/10/12/verifying-distributed-systems-isabelle.html">https://lawrencecpaulson.github.io/2022/10/12/verifying-distributed-systems-isabelle.html</a></li>
</ul>
<h2 id="fallacies">Fallacies</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing</a></li>
<li><a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf">https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf</a></li>
</ul>
<h2 id="errors-and-examples">Errors and Examples</h2>
<ul>
<li><a href="https://github.com/dranov/protocol-bugs-list">https://github.com/dranov/protocol-bugs-list</a></li>
<li><a href="https://covemountainsoftware.com/2021/03/01/a-survey-of-concurrency-bugs/">https://covemountainsoftware.com/2021/03/01/a-survey-of-concurrency-bugs/</a></li>
</ul>
<h2 id="lamport-writings">Lamport writings</h2>
<ul>
<li><a href="http://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos">http://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos</a></li>
</ul>
<h2 id="gfs--google-file-system--storage--dot-dot-dot-dot-dropbox-org-storage-dot-md">GFS (Google File System) - <a href="/posts/storage/">storage</a></h2>
<ul>
<li>Single control plane, master, scared it wouldn&rsquo;t scale but it did</li>
<li>Two specialized operations, record append and snapshot, with other basic file operations
<ul>
<li>record append allows multiple clients to append from small records to a file</li>
<li>snapshot allows the clients to create a copy of a file or a directory tree</li>
</ul>
</li>
<li>batch oriented workloads</li>
<li>commodity servers</li>
<li>Recovery?
<ul>
<li>Checkpoints on metadata state, operation log</li>
<li>shadow managers for recovery to serve read operations only, similar to how RW locks even have contention</li>
</ul>
</li>
<li>Notably, GFS places the consistency checking on the client library, where it requires the you (via the client library) to check</li>
<li>Is <strong>not</strong> linearizable</li>
</ul>
<h3 id="locks">Locks</h3>
<ul>
<li>Read lock aquired on the directory nam (full path to the directory so that it&rsquo;s not being edited or renamed)</li>
<li>Write lock on the file name so that two or more processes cannot write at the same time</li>
<li>Only a small region is locked, since not doing it carefully destroys performance</li>
<li>If a manager dies after locking something, the lock goes with it since it&rsquo;s a central master</li>
<li>left to right locking, so locks are acquired in the shape of
<ul>
<li><code>/a</code>, <code>/a/b</code>, <code>/a/b/c</code> for read locks</li>
<li><code>/a/b/c/file.txt</code> for write locks</li>
</ul>
</li>
</ul>
<h3 id="reading">Reading</h3>
<ol>
<li>To read, user request is first processed by the GFS client that finds a chunk index
<ul>
<li>Since each file is divded into 64MB chunks, the client just %&rsquo;s by 64MB to find the appropriate idx</li>
</ul>
</li>
<li>Then the client requests this information from the manager to get the chunk index and chunk handle, which the manager tells an appropriate replica of</li>
<li>Client then caches the metadata</li>
</ol>
<h3 id="writing">Writing</h3>
<ul>
<li>Two kinds of writes, a <strong>random</strong> write and an <strong>append</strong> (not POSIX compliant)</li>
<li>Replicas hold leases that coordinate the writes</li>
<li>Leases expire, and are only active for a small period of time. This means that if a bad replica has a lease, it will expire and a new node will be found</li>
<li>Primary replicas exist because they allow the data to be consistent</li>
</ul>
<h4 id="writing-workflow">Writing Workflow</h4>
<ul>
<li>Random write and append operations are roughly the same, random writes use an offset for where the data is written, whereas the append just goes to the last chunk</li>
<li>Note that the data is pushed to all the replicas</li>
</ul>
<figure><img src="/ox-hugo/2024-02-18_15-49-47_screenshot.png"/>
</figure>

<ul>
<li>The write (aka commit) happens to the main replica after all the replicas have recieved the data</li>
<li>Replicas then ack whether they&rsquo;ve received the data or not</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>Edge Cases on Writing</p>
<ul>
<li>If the last chunk has available space for appending data, then the chunk servers write that data</li>
<li>If the chunk is already full, then the chunkserver asks the client to create a new chunk, which requires a new write and request loop from the manager</li>
<li>If the last chunk is partially full, the chunkserver holding the last chunk will respond to the client with a message about the available server</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>Control vs Data Flow</p>
<ul>
<li>GFS, critically, decouples the control and the data. The data physically flows via nearest-in-rack, but the manager still runs things</li>
</ul>
<figure><img src="/ox-hugo/2024-02-18_15-54-54_screenshot.png"/>
    </figure>

</li>
</ul>
<h3 id="deletes">Deletes</h3>
<ul>
<li>Garbage collection is done to avoid sync deletes</li>
<li>Client says a delete needs to happen, so master creates a read lock on the dir and a write lock on the file</li>
<li>Master then revokes the leases in progress, and waits for replicas to finish mutations</li>
<li>Afterwards, delete actually happens</li>
</ul>
<h3 id="consistency-model">Consistency Model</h3>
<p>GFS consistency starts from issues during write, we need to prevent writing over multiple times (mix data), and also issues with writing over the same data.</p>
<h4 id="possible-states-after-mutations">Possible States after mutations</h4>
<ul>
<li>Consistent</li>
<li>Inconsistent</li>
<li>Defined - when mutations happen to a file and the applications can parse it and read it</li>
<li>Undefined - when the region hasn&rsquo;t properly changed data yet</li>
</ul>
<h3 id="how-to-actually-maintain-consistency">How to actually maintain consistency?</h3>
<ul>
<li>Random writes can result in serial success, where one lease or lock after the other allows writes to go through</li>
<li>Concurrent success can happen when you have multiple writes on the same thing, resulting in mixed data</li>
<li>Failure</li>
<li>Serial success for appends happen the same as random writes, proper offsets are used</li>
<li>Concurrent success is when two writes attempt to use the same offset</li>
</ul>
<h3 id="dealing-with-data-inconsistencies">Dealing with Data Inconsistencies</h3>
<ul>
<li>It doesn&rsquo;t, you can have overlapping writes, the application level is responsible for locking</li>
<li>For append however, appends happen as at least one atomic unit.</li>
<li>GFS only deals with stale data because of bad replications</li>
</ul>
<h3 id="metadata-consistency">Metadata Consistency</h3>
<ul>
<li>Chunkservers tell the metadata manager what offsets map to where, but the manager also keeps track of it</li>
<li>We also use shadow managers when the client cannot reach the primary manager, which help facilitate primary reads
<ul>
<li>also the shadow might be behind the primary, since it reads off of the master&rsquo;s logs</li>
</ul>
</li>
<li>Master is also fully syncronous, it does not respond to the client&rsquo;s requests for updating metadata until everything is done</li>
<li>Failures can be retried by the client side, or present in the operation log</li>
</ul>
<h3 id="scalability">Scalability</h3>
<ul>
<li>Scalability is achieved mostly through having multiple chunkservers, which can be easily added</li>
<li>Availability is achieved by having three chunk servers per app by default</li>
<li>Replication happens in the background if hosts are lost, and GFS manager has shadow managers that take over should the master go down</li>
<li>Durability is acheived that there&rsquo;s replicas for chunkservers and operation logs for the metadata</li>
<li>Throughput is achieved by separating the data from the metadata flow</li>
<li>Consistency is applied through relaxed consistency, most files are mutated by having append and reads</li>
</ul>
<h2 id="colossus">Colossus</h2>
<ul>
<li>GFS single master could not hold everything</li>
<li>Reed Solomon vs full replication
<ul>
<li>parity bits are added for error correction, whereas full replication only uses</li>
</ul>
</li>
<li>Frontend has changed
<ul>
<li>Curator -&gt; horizontally scalable metadata servers</li>
<li>Master metadata database</li>
<li>Custodians -&gt; horizontal scalers</li>
</ul>
</li>
<li>GFS was built on the assumption that file sizes were always going to outnumber metadata, but the biggest problem is that that may not be true, small blobs double their size with metadata</li>
</ul>
<h3 id="client-library">Client Library</h3>
<ul>
<li>Still the most complex part, plays the same role as the GFS client</li>
</ul>
<h3 id="control-plane-and-metadata-database">Control Plane and Metadata Database</h3>
<ul>
<li>Metadata Database has moved to BigTable, but curators fill the role of serving metadata (horizonitally scalable metadata servers), and custodians manage the data (by moving files around), D file servers retain the same process as mapping over disks</li>
</ul>
<h2 id="tectonic">Tectonic</h2>
<ul>
<li>Facebook version of GFS, uses <a href="/posts/k_v_stores/#zippydb">ZippyDB</a> to handle async metadata management and block management</li>
<li><strong>Only allows one single writer at a time</strong> via locking</li>
<li>Each block is 80MB</li>
<li>Organizes the shards in zippy so that all the ones with the same sharding ID go on the same replica, which uses the <code>directory_id</code></li>
<li>Remote chunk store that actually stores the physical bytes</li>
<li>Has a cache note called &ldquo;sealing&rdquo;, which restraints updates, allowing caches to be stored longer</li>
<li>Has a 2pc for rebalancing when nodes need to go down for maintanence</li>
<li>Divides traffic groups into 3, depending on priority</li>
<li>Uses the <a href="/posts/load_balancing/#leaky-bucket-algorithm">Leaky Bucket Algorithm</a></li>
</ul>
<h3 id="multitenancy">Multitenancy</h3>
<ul>
<li>Tectonic has quota limits for every user</li>
<li>Also allows applications to mark itself in the traffic groups, similar to <code>nice</code> values in <a href="/posts/linux/">linux</a></li>
<li>Tectonic distinguishes between emphermal and non-emphemeral resources for traffic sharding
<ul>
<li>Emphemeral is IOPS and metadata query capacity, stuff that changes in real time</li>
</ul>
</li>
</ul>
<h2 id="bigtable">BigTable</h2>
<ul>
<li>Key features
<ul>
<li>Single row transactions</li>
<li>No batching across row keys</li>
<li>Integer counters</li>
<li>MapReduce jobs as input source and output target</li>
</ul>
</li>
<li>Uses a column family style, with groups of columns and a timestamp within each cell</li>
<li>Treats all data as raw byte strings</li>
<li>All rows have a row key (64kb string), each write and read is atomic</li>
<li>Column families allow for efficient read/write operations, since they usually hold data of the same type (data within a family is physically close)</li>
<li>Also has timestamps, keeps the latest 3 versions</li>
</ul>
<h3 id="sstable--sorted-string-table">SSTable (Sorted String Table)</h3>
<ul>
<li>Components
<ul>
<li>Data File</li>
<li>Primary Index</li>
<li>Bloom filter (for quickly checking if it&rsquo;s in the data structure)</li>
<li>Compression information</li>
<li>Stats</li>
</ul>
</li>
<li>Immutable k/v mappings used to represent tablet states, consisting of blocks, with block indexes facilitating efficient disk access, and can be loaded into memory for quick queries</li>
<li>Memtables are created in memory and flushed to disk as sstables</li>
</ul>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/2021_goals/">2021 goals</a></li>
       
          <li><a href="/posts/actor_model/">actor model</a></li>
       
          <li><a href="/posts/broadcasting_and_multi_cast/">broadcasting and multicast</a></li>
       
          <li><a href="/posts/bully_algorithm/">bully algorithm</a></li>
       
          <li><a href="/posts/byzantine_fault_tolerance_bft/">byzantine fault tolerance (BFT)</a></li>
       
          <li><a href="/posts/cap_theorm/">cap theorm (brewer&#39;s conjecture and the feasibility of consistent, available, partition-tolerant web services</a></li>
       
          <li><a href="/posts/20200415152613-cassandra/">Cassandra</a></li>
       
          <li><a href="/posts/clocks_distributed_systems/">clocks (distributed systems)</a></li>
       
          <li><a href="/posts/crdts/">crdts</a></li>
       
          <li><a href="/posts/dataflow/">dataflow</a></li>
       
          <li><a href="/posts/distributed_systems_for_fun_and_profit/">distributed systems for fun and profit</a></li>
       
          <li><a href="/posts/distributed_transactions/">distributed transactions</a></li>
       
          <li><a href="/posts/epaxos/">epaxos</a></li>
       
          <li><a href="/posts/gossip_protocols/">gossip protocols</a></li>
       
          <li><a href="/posts/gray_failures/">gray failures</a></li>
       
          <li><a href="/posts/homogenous_time_blog_post/">Homogenous Time: Ibn Khaldun and Transaction Logs</a></li>
       
          <li><a href="/posts/impossibility_of_consensus_with_one_faulty_process/">impossibility of consensus with one faulty process</a></li>
       
          <li><a href="/posts/kubernentes/">kubernetes (k8s)</a></li>
       
          <li><a href="/posts/load_balancing/">load balancing</a></li>
       
          <li><a href="/posts/metastable_failures_blog_post/">metastable failures blog post</a></li>
       
          <li><a href="/posts/paxos/">paxos</a></li>
       
          <li><a href="/posts/physalia_amazon_ebs/">physalia - amazon ebs</a></li>
       
          <li><a href="/posts/raft/">raft</a></li>
       
          <li><a href="/posts/windows_failover_clusters_hci/">windows failover clusters (hci)</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>
</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
