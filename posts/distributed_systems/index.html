<!DOCTYPE html>
<html><title>distributed systems</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.4b9309a2ca0781957619d4e9500f6539479ebf8cdcbfaac702d6364b30e2c87c.css"/>
<script defer src="/en.search.min.d591dcdd0b08ff54d09cd1e705d9f4aab451e3673238bfb0eaa1705915e83823.js" integrity="sha256-1ZHc3QsI/1TQnNHnBdn0qrRR42cyOL&#43;w6qFwWRXoOCM="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>distributed systems</h1>

          <p>Tags: <a href="/posts/computers/">computers</a></p>
<ul>
<li><a href="http://muratbuffalo.blogspot.com/2021/02/foundational-distributed-systems-papers.html">http://muratbuffalo.blogspot.com/2021/02/foundational-distributed-systems-papers.html</a></li>
<li><a href="https://github.com/stevana/armstrong-distributed-systems/tree/main">https://github.com/stevana/armstrong-distributed-systems/tree/main</a></li>
</ul>
<h2 id="short-introduction">Short Introduction</h2>
<ul>
<li><a href="http://book.mixu.net/distsys/single-page.html">http://book.mixu.net/distsys/single-page.html</a></li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628</a></li>
</ul>
<h2 id="system-design">System Design</h2>
<h3 id="high-level-methodology">High Level Methodology</h3>
<ul>
<li><a href="https://static.googleusercontent.com/media/sre.google/en//static/pdf/nalsd-workbook-letter.pdf">https://static.googleusercontent.com/media/sre.google/en//static/pdf/nalsd-workbook-letter.pdf</a></li>
<li>Do one thing and do it well
<ul>
<li>Services should be split up into ones that do one thing, makes performance analysis easier and easier for the system to evolve over time</li>
</ul>
</li>
<li>Prefer simple, stateless services where possible
<ul>
<li>State must be kept, but try to reduce the surface area of the state. Stateless components are easy to manage because they&rsquo;re trivial to scale horizontally</li>
</ul>
</li>
<li>Assumptions, constraints, and SLO&rsquo;s
<ul>
<li>Before you start designing, make sure to look at the constraints and non-functional requirements for your system. Have they been state? Are they comprehensive?</li>
<li>At minimum you should understand
<ul>
<li>What the system is supposed to do (business logic)</li>
<li>What the SLOs are around performance, error rates/availability, data loss? If no SLOs, you made need to define them with business owners</li>
<li>Where should it run? What is the budget or what hardware is available?</li>
<li>Expected workload (this may be difficult to gauge)</li>
</ul>
</li>
</ul>
</li>
<li>Getting to an initial high level design (or analysing an existing one)
<ul>
<li>API
<ul>
<li>You need to figure out the API for the system. What operations are available externally? What data goes in, and what results comes out? Do operatinos change the system state, or is it read-only?</li>
<li>Each set of closely related operations is likely to be a service in the system. Many systems have a stateless API layer that coordinates with other parts of the system to serve results</li>
</ul>
</li>
<li>Follow the data
<ul>
<li>After the API, figure out the data flow. What state does the system need to read and/or write in order to do the work? What do we know about the state?</li>
</ul>
</li>
</ul>
</li>
<li>Data consistency - critical data
<ul>
<li>Some applications require very consistent views of critical state</li>
<li>This should be tackled with distributed consensus algorithms to manage state changes, 3/5/+ replicas in different failure domains</li>
<li>Fastpaxos uses a stable leader process to improve performance. Performance for FastPaxos is one rtt between the leader and qurom of closest replicas to commit and update or a consistent read. The client must communicate with the leader, so the percieved latency for the client is the RTT between client/leader + leader/nearest quorum. Stale reads require a roundtrip to the nearest healthy replica, but the data may not be consistent. Other transactions can be batched</li>
</ul>
</li>
<li>Data consistency - less critical data
<ul>
<li>Not all data needs to highly consistent, in these cases replication + backups is often enough.</li>
</ul>
</li>
<li>Data storage considerations
<ul>
<li>Lots of things to think about when storing data, most fundalmental is where are you going to organize the data in RAM and disk</li>
<li>Think about how data is read and written. Which is the most common use case? Usually it&rsquo;s reading.</li>
<li>Think about storing data to match the most common use case. If you&rsquo;re reading in a chronological order by user, consider storing it like that</li>
</ul>
</li>
<li>Sharding data
<ul>
<li>Data obviously needs to be sharded, but watch out for hot shards</li>
<li>Easiest way to shard is m shardes on n servers, where m &gt; 100x n</li>
<li>shard = hash(key) modulko m</li>
<li>we have a map of shard -&gt; server that we look up to find the server</li>
<li>the system can dynamically move shard and update the map in order to rebalance, since shards are relatively small</li>
<li>sharded and replicated datastores should have automatic mechanisms that cross-check state with other replicas and load lost state from peers, useful after downtime and when new replicas are added. Note that this should also be rate-limited somehow to avoid thundering herd problems where many replicas are out of sync</li>
</ul>
</li>
<li>Scaling and Performance
<ul>
<li>Make sure to do some back of the envelope calculations to estimate the potential performance of the system, any scaling limits created by the bottlenecks in the system, and how many machines are likely to be needed to service the given workload</li>
<li>Calculations to use
<ul>
<li>Disk - sanity check the volume of data read/written, disk seeks over time</li>
<li>RAM - how much can we cache? Are we storing an index of some kind?</li>
<li>Bandwidth - size of requests inwards and outwards, do we have enough bandwidth?
<ul>
<li>Bandwidth between datacenters and bandwidth between machines in the same DC</li>
</ul>
</li>
<li>CPU - is this a computationally intensive service? - this can be hard to gauge</li>
<li>What are the estimated concurrent transactions for a service (compute based on throughput per second and latency, e.g. 500 requests per second, 200ms average latency -&gt; 500/(1000/200) -&gt; 100 transactions in flight) - is this a reasonable number? Each request needs some RAM to manage state and some CPU to process</li>
</ul>
</li>
</ul>
</li>
<li>Iterating and refining
<ul>
<li>After a complete design, time to take another look. You might&rsquo;ve unearthed new constraints</li>
<li>Consider
<ul>
<li>Does it work correctly?</li>
<li>Are you meeting your SLOs and constraints?</li>
<li>Is it as simple as it could be?</li>
<li>Is it reliable enough?</li>
<li>Is it making efficient use of hardware?</li>
<li>What assumptions did you make and are there risks?</li>
</ul>
</li>
<li>Are there places you can improve caching, indexing, lookup filters like Bloom filters? Can you degrade gracefully when overloaded by doing cheaper operations?</li>
</ul>
</li>
<li>Monitoring
<ul>
<li>Montiring should be included in the design phase of any distributed system</li>
<li>It needs to
<ul>
<li>Detect outages (and alert)</li>
<li>Provide information on whether you&rsquo;re meeting your SLOs</li>
<li>Give indications of growth and system capacity</li>
<li>Assist in troubleshooting outages and less critical problems</li>
</ul>
</li>
<li>Design metrics for your system and indicate how you will collect data. What are your alert thresholds/conditions?</li>
</ul>
</li>
<li>What could go wrong?
<ul>
<li>Answer these questions
<ul>
<li>What is the scariest thing that can happen to your system?</li>
<li>How does your system defend against it?</li>
<li>Will it continue to meet its SLO? How can you recover, if not?</li>
<li>Can improvements be made to the system to prevent the problem, and what would they cost?</li>
</ul>
</li>
<li>Consider
<ul>
<li>Loss of a data center</li>
<li>Loss of important state/data</li>
<li>Bugs in critical software component</li>
<li>Load spikes</li>
<li>Queries/requests of death that can crash the process</li>
<li>10x growth in workload - will it scale?</li>
<li>Change in data access patterns changes cache hit rate</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="pub-sub">Pub Sub</h3>
<h2 id="theory">Theory</h2>
<ul>
<li><a href="https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/">https://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/</a></li>
</ul>
<h3 id="verification">Verification</h3>
<ul>
<li><a href="https://lawrencecpaulson.github.io/2022/10/12/verifying-distributed-systems-isabelle.html">https://lawrencecpaulson.github.io/2022/10/12/verifying-distributed-systems-isabelle.html</a></li>
</ul>
<h2 id="fallacies">Fallacies</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing</a></li>
<li><a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf">https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf</a></li>
</ul>
<h2 id="errors-and-examples">Errors and Examples</h2>
<ul>
<li><a href="https://github.com/dranov/protocol-bugs-list">https://github.com/dranov/protocol-bugs-list</a></li>
<li><a href="https://covemountainsoftware.com/2021/03/01/a-survey-of-concurrency-bugs/">https://covemountainsoftware.com/2021/03/01/a-survey-of-concurrency-bugs/</a></li>
</ul>
<h2 id="lamport-writings">Lamport writings</h2>
<ul>
<li><a href="http://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos">http://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos</a></li>
</ul>
<h2 id="gfs--google-file-system--storage--dot-dot-dot-dot-dropbox-org-storage-dot-md">GFS (Google File System) - <a href="/posts/storage/">storage</a></h2>
<ul>
<li>Single control plane, master, scared it wouldn&rsquo;t scale but it did</li>
<li>Two specialized operations, record append and snapshot, with other basic file operations
<ul>
<li>record append allows multiple clients to append from small records to a file</li>
<li>snapshot allows the clients to create a copy of a file or a directory tree</li>
</ul>
</li>
<li>batch oriented workloads</li>
<li>commodity servers</li>
<li>Recovery?
<ul>
<li>Checkpoints on metadata state, operation log</li>
<li>shadow managers for recovery to serve read operations only, similar to how RW locks even have contention</li>
</ul>
</li>
<li>Notably, GFS places the consistency checking on the client library, where it requires the you (via the client library) to check</li>
<li>Is <strong>not</strong> linearizable</li>
</ul>
<h3 id="locks">Locks</h3>
<ul>
<li>Read lock aquired on the directory nam (full path to the directory so that it&rsquo;s not being edited or renamed)</li>
<li>Write lock on the file name so that two or more processes cannot write at the same time</li>
<li>Only a small region is locked, since not doing it carefully destroys performance</li>
<li>If a manager dies after locking something, the lock goes with it since it&rsquo;s a central master</li>
<li>left to right locking, so locks are acquired in the shape of
<ul>
<li><code>/a</code>, <code>/a/b</code>, <code>/a/b/c</code> for read locks</li>
<li><code>/a/b/c/file.txt</code> for write locks</li>
</ul>
</li>
</ul>
<h3 id="reading">Reading</h3>
<ol>
<li>To read, user request is first processed by the GFS client that finds a chunk index
<ul>
<li>Since each file is divded into 64MB chunks, the client just %&rsquo;s by 64MB to find the appropriate idx</li>
</ul>
</li>
<li>Then the client requests this information from the manager to get the chunk index and chunk handle, which the manager tells an appropriate replica of</li>
<li>Client then caches the metadata</li>
</ol>
<h3 id="writing">Writing</h3>
<ul>
<li>Two kinds of writes, a <strong>random</strong> write and an <strong>append</strong> (not POSIX compliant)</li>
<li>Replicas hold leases that coordinate the writes</li>
<li>Leases expire, and are only active for a small period of time. This means that if a bad replica has a lease, it will expire and a new node will be found</li>
<li>Primary replicas exist because they allow the data to be consistent</li>
</ul>
<h4 id="writing-workflow">Writing Workflow</h4>
<ul>
<li>Random write and append operations are roughly the same, random writes use an offset for where the data is written, whereas the append just goes to the last chunk</li>
<li>Note that the data is pushed to all the replicas</li>
</ul>
<figure><img src="/ox-hugo/2024-02-18_15-49-47_screenshot.png"/>
</figure>

<ul>
<li>The write (aka commit) happens to the main replica after all the replicas have recieved the data</li>
<li>Replicas then ack whether they&rsquo;ve received the data or not</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>Edge Cases on Writing</p>
<ul>
<li>If the last chunk has available space for appending data, then the chunk servers write that data</li>
<li>If the chunk is already full, then the chunkserver asks the client to create a new chunk, which requires a new write and request loop from the manager</li>
<li>If the last chunk is partially full, the chunkserver holding the last chunk will respond to the client with a message about the available server</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>Control vs Data Flow</p>
<ul>
<li>GFS, critically, decouples the control and the data. The data physically flows via nearest-in-rack, but the manager still runs things</li>
</ul>
<figure><img src="/ox-hugo/2024-02-18_15-54-54_screenshot.png"/>
    </figure>

</li>
</ul>
<h3 id="deletes">Deletes</h3>
<ul>
<li>Garbage collection is done to avoid sync deletes</li>
<li>Client says a delete needs to happen, so master creates a read lock on the dir and a write lock on the file</li>
<li>Master then revokes the leases in progress, and waits for replicas to finish mutations</li>
<li>Afterwards, delete actually happens</li>
</ul>
<h3 id="consistency-model">Consistency Model</h3>
<p>GFS consistency starts from issues during write, we need to prevent writing over multiple times (mix data), and also issues with writing over the same data.</p>
<h4 id="possible-states-after-mutations">Possible States after mutations</h4>
<ul>
<li>Consistent</li>
<li>Inconsistent</li>
<li>Defined - when mutations happen to a file and the applications can parse it and read it</li>
<li>Undefined - when the region hasn&rsquo;t properly changed data yet</li>
</ul>
<h3 id="how-to-actually-maintain-consistency">How to actually maintain consistency?</h3>
<ul>
<li>Random writes can result in serial success, where one lease or lock after the other allows writes to go through</li>
<li>Concurrent success can happen when you have multiple writes on the same thing, resulting in mixed data</li>
<li>Failure</li>
<li>Serial success for appends happen the same as random writes, proper offsets are used</li>
<li>Concurrent success is when two writes attempt to use the same offset</li>
</ul>
<h3 id="dealing-with-data-inconsistencies">Dealing with Data Inconsistencies</h3>
<ul>
<li>It doesn&rsquo;t, you can have overlapping writes, the application level is responsible for locking</li>
<li>For append however, appends happen as at least one atomic unit.</li>
<li>GFS only deals with stale data because of bad replications</li>
</ul>
<h3 id="metadata-consistency">Metadata Consistency</h3>
<ul>
<li>Chunkservers tell the metadata manager what offsets map to where, but the manager also keeps track of it</li>
<li>We also use shadow managers when the client cannot reach the primary manager, which help facilitate primary reads
<ul>
<li>also the shadow might be behind the primary, since it reads off of the master&rsquo;s logs</li>
</ul>
</li>
<li>Master is also fully syncronous, it does not respond to the client&rsquo;s requests for updating metadata until everything is done</li>
<li>Failures can be retried by the client side, or present in the operation log</li>
</ul>
<h3 id="scalability">Scalability</h3>
<ul>
<li>Scalability is achieved mostly through having multiple chunkservers, which can be easily added</li>
<li>Availability is achieved by having three chunk servers per app by default</li>
<li>Replication happens in the background if hosts are lost, and GFS manager has shadow managers that take over should the master go down</li>
<li>Durability is acheived that there&rsquo;s replicas for chunkservers and operation logs for the metadata</li>
<li>Throughput is achieved by separating the data from the metadata flow</li>
<li>Consistency is applied through relaxed consistency, most files are mutated by having append and reads</li>
</ul>
<h2 id="colossus">Colossus</h2>
<ul>
<li>GFS single master could not hold everything</li>
<li>Reed Solomon vs full replication
<ul>
<li>parity bits are added for error correction, whereas full replication only uses</li>
</ul>
</li>
<li>Frontend has changed
<ul>
<li>Curator -&gt; horizontally scalable metadata servers</li>
<li>Master metadata database</li>
<li>Custodians -&gt; horizontal scalers</li>
</ul>
</li>
<li>GFS was built on the assumption that file sizes were always going to outnumber metadata, but the biggest problem is that that may not be true, small blobs double their size with metadata</li>
</ul>
<h3 id="client-library">Client Library</h3>
<ul>
<li>Still the most complex part, plays the same role as the GFS client</li>
</ul>
<h3 id="control-plane-and-metadata-database">Control Plane and Metadata Database</h3>
<ul>
<li>Metadata Database has moved to BigTable, but curators fill the role of serving metadata (horizonitally scalable metadata servers), and custodians manage the data (by moving files around), D file servers retain the same process as mapping over disks</li>
</ul>
<h2 id="tectonic">Tectonic</h2>
<ul>
<li>Facebook version of GFS, uses <a href="/posts/k_v_stores/#zippydb">ZippyDB</a> to handle async metadata management and block management</li>
<li><strong>Only allows one single writer at a time</strong> via locking</li>
<li>Each block is 80MB</li>
<li>Organizes the shards in zippy so that all the ones with the same sharding ID go on the same replica, which uses the <code>directory_id</code></li>
<li>Remote chunk store that actually stores the physical bytes</li>
<li>Has a cache note called &ldquo;sealing&rdquo;, which restraints updates, allowing caches to be stored longer</li>
<li>Has a 2pc for rebalancing when nodes need to go down for maintanence</li>
<li>Divides traffic groups into 3, depending on priority</li>
<li>Uses the <a href="/posts/load_balancing/#leaky-bucket-algorithm">Leaky Bucket Algorithm</a></li>
</ul>
<h3 id="multitenancy">Multitenancy</h3>
<ul>
<li>Tectonic has quota limits for every user</li>
<li>Also allows applications to mark itself in the traffic groups, similar to <code>nice</code> values in <a href="/posts/linux/">linux</a></li>
<li>Tectonic distinguishes between emphermal and non-emphemeral resources for traffic sharding
<ul>
<li>Emphemeral is IOPS and metadata query capacity, stuff that changes in real time</li>
<li>Non-emphemeral is non-sharable resources that are once allocated to a tenant and will not be allocated to another, like storage</li>
</ul>
</li>
<li>TrafficGroups is an emphemeral sharing group, which is a collection of clients that have the same latency and IOPS requirements</li>
<li>There&rsquo;s 50 TrafficGroups per cluster, although this is configurable.</li>
<li>Each tenant allocates the require emphermal resources based on TraffiCgroup and TrafficClasses (gold, silver, bronze)
<ul>
<li>unused emphemeral resources are shared with the traffic group within the tenant of a high traffic class</li>
</ul>
</li>
<li>Client is actually responsible for this, the client library will check for spare capacity that is available within the same traffic group, then different traffic group of the same tenant, and finally spare capacity in the different tenants with the same TrafficClass priority</li>
<li>Optimizations used by storage nodes
<ul>
<li>Avoiding local hotspots is done with a weight round robin</li>
<li>use a greedy optimiation for allowing low-latency requests to give up their turn for the high traffic class if the request can be completed after the completion of the high traffic class</li>
<li>limits are set for all non-gold requests</li>
<li>we have enough disks to rearrange the IO requests</li>
</ul>
</li>
<li>Authentication is done via a token based system</li>
</ul>
<h3 id="tenant-specific-optimizations">Tenant Specific Optimizations</h3>
<ul>
<li>Data warehouse: write once, read many
<ul>
<li>RS (<a href="/posts/reed_solomon_encoding/">reed-solomon encoding</a>) encoded async writes
<ul>
<li>RS encoding happens out of sync after the stuff is written</li>
</ul>
</li>
<li>Hedged quorum writes - generating reservation requests
<ul>
<li>Improves tail latency</li>
<li>Rather than sending the chunk to write further on the storage nodes, it sends only reservation requests, which then tell the nodes to accept the request of the reservation</li>
<li>So if you have 5 nodes, you send out to 5, and just write to the 3 that respond first</li>
</ul>
</li>
</ul>
</li>
<li>Blob storage (low latency, write many, read many, typically smaller than block size)
<ul>
<li>Consistent append on partial blocks
<ul>
<li>Whenever a small blob comes in, since they&rsquo;re usually smaller than a block, we can just append to the block and perform replication on the partial block only</li>
<li>To prevent partial block issues, there&rsquo;s a lock on the writer</li>
</ul>
</li>
<li>Re-encoding blocks
<ul>
<li>RS encoding on full blocks is IO efficient, but partial block is inefficient. Save the RS encoding until a full block has been written, and seal the block from further mutation, similar to SS tables</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="bigtable">BigTable</h2>
<ul>
<li>Key features
<ul>
<li>
<p>Single row transactions</p>
</li>
<li>
<p>No batching across row keys</p>
</li>
<li>
<p>Integer counters</p>
</li>
<li>
<p>MapReduce jobs as input source and output target</p>
<figure><img src="/ox-hugo/2024-02-21_16-32-56_screenshot.png"/>
        </figure>

</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>Uses a column family style, with groups of columns and a timestamp within each cell</li>
<li>Treats all data as raw byte strings</li>
<li>All rows have a row key (64kb string), each write and read is atomic</li>
<li>Column families allow for efficient read/write operations, since they usually hold data of the same type (data within a family is physically close)</li>
<li>Also has timestamps, keeps the latest 3 versions</li>
</ul>
<h3 id="components">Components</h3>
<h4 id="memtable">Memtable</h4>
<ul>
<li>Recent modifications in an in-memory, mutable sorted buffer</li>
<li>When a memtable grows big enough, its gets flushed to disk as an SSTable</li>
<li>Uses a write-ahead-log for this</li>
</ul>
<h4 id="sstable--sorted-string-table">SSTable (Sorted String Table)</h4>
<ul>
<li>Components
<ul>
<li>Data File</li>
<li>Primary Index</li>
<li>Bloom filter (for quickly checking if it&rsquo;s in the data structure)</li>
<li>Compression information</li>
<li>Stats</li>
<li>64kb blocks</li>
</ul>
</li>
<li>Immutable k/v mappings used to represent tablet states, consisting of blocks, with block indexes facilitating efficient disk access, and can be loaded into memory for quick queries</li>
</ul>
<h4 id="chubby">Chubby</h4>
<ul>
<li>Highly available and persistent distributed lock service</li>
<li>typically runs with five operating replicas, similar to zoo</li>
<li>Used in BigTable to ensure there&rsquo;s only one operational manager</li>
<li>Saves a bootstrap information as well as new tablet servers and failures</li>
<li>ACLs too</li>
</ul>
<h3 id="operations">Operations</h3>
<ul>
<li>Locating tablets for a piece of data
<ul>
<li>Uses a three level structure similar to a B+ tree</li>
<li>root tablet&rsquo;s location is stored in a Chubby file</li>
<li>second tier contains all metadata tablets</li>
<li>third tier contains all user tablets</li>
</ul>
</li>
<li>The root tablet is a pointer to other tablets</li>
<li>first tablet in the metadata data is the root tablet, which is treated differently</li>
<li>We cache tablet positions in the client library</li>
<li>Client can then check the metadata server, which then goes to chubby, to get info</li>
<li>Basically the thing to know is that this is similar to an inode, where there&rsquo;s three layers of indirection</li>
<li>A tablet is assigned to just one tablet server, the manager also maintains a record of unassigned tablets and allocates them to tablet servers that have enough space</li>
<li>Manager requires heartbeats and locks via chubby to assure that a tablet server is holding a tablet</li>
<li>New managers interact with each active tablet server after looking them up in the tablet server directory in chubby</li>
</ul>
<h4 id="writes">Writes</h4>
<ul>
<li>When a write is received by a tablet server, it gets checked to ensure to ensure data validity and proper formatting
<ul>
<li>check authorizations in the lock</li>
</ul>
</li>
</ul>
<h4 id="reads">Reads</h4>
<ul>
<li>Same thing as write, initial check is via authorization in chubby, and then loads the SS tables</li>
</ul>
<h3 id="minor-merging-and-major-compactions">Minor, Merging, and Major Compactions</h3>
<ul>
<li>Minor compaction is when it flushes the memtable to disk and turns it into an sstable</li>
<li>Merging compaction is when sstables become too many and we merge them together, although this could include deleted entries</li>
<li>Major compaction is when we merge multiple sstables together and remove the deleted datace the tombstones are handled</li>
</ul>
<h3 id="design-refinements">Design Refinements</h3>
<ul>
<li>Column family locality
<ul>
<li>compression is also done on the column family locality to store data</li>
</ul>
</li>
<li>Tablet caches in two ways
<ul>
<li>Scan cache - high level cache that stores k/v given to the tablet server</li>
<li>Block cache - caches blocks, like page caching</li>
</ul>
</li>
<li>Bloom filters
<ul>
<li>Bloom filters tell you 100% if it is <strong>not</strong> in the table.</li>
<li>Quickly check which things are in a sstable</li>
</ul>
</li>
<li>Commit logs
<ul>
<li>There&rsquo;s only one big commit log</li>
<li>Recovering servers must load off of another&rsquo;s commit log</li>
<li>Duplicate log reads are by sorting the entries by <code>&lt;table, row name, log seq number&gt;</code>.</li>
</ul>
</li>
<li>Speeding up tablet recovery
<ul>
<li>Most challenging and time consuming jobs is ensuring the tablet server gets all entries from the commit log</li>
<li>Minor compactions are done before moving to reduce the amount of stuff needed to move.</li>
</ul>
</li>
</ul>
<h3 id="system-design-wisdom">System Design Wisdom</h3>
<ul>
<li>Uses other services (Chubby and GFS) as building blocks</li>
<li>Locality hints from the users for many of its optimizations, such as column families</li>
<li>One log file per tablet is a way to allow individual tablets to recover</li>
<li>Single manager interface</li>
</ul>
<h2 id="megastore">Megastore</h2>
<ul>
<li>SQL based system</li>
<li>Built on top of bigtable</li>
<li>Main goal was to facilitate ACID transactions</li>
</ul>
<h3 id="design">Design</h3>
<ul>
<li>Application server
<ul>
<li>Used to deploy emgastore, local replica on each application server, which writes to the local bigtable</li>
</ul>
</li>
<li>Megastore library
<ul>
<li>Connects and implements paxos and replica picking</li>
</ul>
</li>
<li>Replication server
<ul>
<li>Checks for unfinished writes, similar to a garbage collector, which provides paxos no-op commands</li>
</ul>
</li>
<li>Coordinator
<ul>
<li>Keeps track of entity groups which its replicas has seen every Paxos write</li>
</ul>
</li>
<li>Bigtable
<ul>
<li>Handles arbitrary reads and write throughputs</li>
</ul>
</li>
</ul>
<figure><img src="/ox-hugo/2024-02-22_19-02-48_screenshot.png"/>
</figure>

<h3 id="replication-strats">Replication Strats</h3>
<ul>
<li>Async primary/secondary - write ahead logs are replciated to at least one secondary node, but log appends are recognized and serialized at the primary node while being sent down to the secondary nodes. Primary is responsible for ACID, but it is acceptable to have a node fail and a secondary take on a primary.</li>
<li>Sync primary/secondary, primary node waits until its actually been replicated before doing a tx</li>
<li>Optimistic replication, where no primary, and any node can accept the changes and propagate them async to other nodes. Better availability and latency, but transactions are not possible.</li>
</ul>
<h3 id="paxos">Paxos</h3>
<ul>
<li>Megastore implements paxos, which has issues being slow</li>
<li>But Paxos gives
<ul>
<li>NO primary but a group</li>
<li>WAL is replciated to all nodes</li>
<li>Any node can start read/write</li>
<li>Every log adds the changes only if a majority acks</li>
<li>Remaining nodes eventually catch up</li>
<li>Communication is hindered if a majority is down</li>
</ul>
</li>
<li>Entity groups are used to replicate</li>
</ul>
<figure><img src="/ox-hugo/2024-02-22_19-06-43_screenshot.png"/>
</figure>

<ul>
<li>In entity groups, entities are changed using single-phase ACID transactions, which paxos uses to replicate the common record
<ul>
<li>2pc can also be used</li>
</ul>
</li>
<li>Megastore async replicates between different entity groups. Local indexes have strong consistenty, but global ones have weak consistenty</li>
</ul>
<figure><img src="/ox-hugo/2024-02-22_19-14-17_screenshot.png"/>
</figure>

<h3 id="megastore-data-model">Megastore Data Model</h3>
<ul>
<li>API design
<ul>
<li>Relatively stable performance benefits</li>
<li>Shift work from read time to write time since there&rsquo;s definitely going to be more reads than writes</li>
</ul>
</li>
<li>Data model is strongly typed, similar to RDMS, where schemas and a collection of tables are named and typed.</li>
<li>Key distinctions: root tables and child tables
<ul>
<li>Child table needs to specificy a foreign key referring to a root table, and a root table and all its child tables make up an entity group</li>
</ul>
</li>
<li>Prejoining with keys
<ul>
<li>normally, primary keys have surrogate values that idenitfy each row in a table</li>
</ul>
</li>
<li>Two layers of secondary indexes
<ul>
<li>Local index: each group&rsquo;s local index is used as a distinct index, which locates data inside an entity group</li>
<li>Global index: encompasses many entity groups, used to locate entities without knowing which entity groups include them in advance</li>
</ul>
</li>
<li>All of this is splayed on top of big table to get atomicity, consistency, isolation, and durability
<ul>
<li>Uses the versioning feature of bigtable to implement multi-version concurrency control</li>
</ul>
</li>
<li>Read consistency
<ul>
<li>Gives you three levels
<ul>
<li>Current: makes sure all committed writes have been executed, and does a read</li>
<li>Snapshot: reads the latest committed write operation</li>
<li>Inconsistent: reads the most recent value without considering the log&rsquo;s state</li>
</ul>
</li>
</ul>
</li>
<li>Writes
<ul>
<li>Writes do a current read before doing a write so it can find the latest offset for the log, then uses paxos to get all nodes to buy in, then commits it and updates it in bigtable</li>
</ul>
</li>
<li>Queues
<ul>
<li>Transactional messaging among groups of entities are handled through queues, which is used for batching many changes into a single transaction</li>
</ul>
</li>
</ul>
<h3 id="replication-in-megastore">Replication in Megastore</h3>
<ul>
<li>Paxos WAL is mostly where it sits</li>
<li>Also allows local reads from anywhere, since paxos is used to make sure everyone is together</li>
<li>Each log position is a paxos usage, which optimizes to skip the preparation phase and enter the acceptance phase when the same proposer makes continous proposals</li>
<li>Leader decides which values are allowed ot use proposal 0</li>
<li>Different types of replicas
<ul>
<li>Witness replicas
<ul>
<li>Have WALs and vote in Paxos</li>
<li>Reduced storage costs because they don&rsquo;t actually store the data</li>
<li>Are able to avoid the need for an additional round trip when unable write</li>
</ul>
</li>
<li>Read only replicas
<ul>
<li>Can&rsquo;t vote</li>
<li>Contain snapshots of the data</li>
<li>Read only replicas can distribute the data over large geographic area, basically a CDN</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="replicated-logs">Replicated Logs</h4>
<ul>
<li>Replicated logs have many shapes, but we also allow holes in the logs for when consensus was not found</li>
<li>Catching up
<ul>
<li>Node checks if the entity group is up-to-date locally by contacting the coordinator of the local replica</li>
<li>Then finds the position, which detemrines and picks a replica that has applied through the highest possibly commited log position</li>
<li>Catches up by reading the values from the different replicas. Empty no-op values for ones that are disputed, Paxos then forces all the majority of the replicas to accept the no-op or previously comimtted write</li>
</ul>
</li>
<li>Coordinators oversee the entire process, and they themselves are locked in Chubby</li>
</ul>
<h2 id="scaling-memcache">Scaling Memcache</h2>
<h3 id="set-get-delete-operations"><code>set</code>, <code>get</code>, <code>delete</code> operations</h3>
<h3 id="main-data-structures-of-memcache">Main data structures of memcache</h3>
<ul>
<li>
<p>Hash table</p>
<ul>
<li>Chained hash table, everything is a linked list</li>
</ul>
</li>
<li>
<p>Cache item data structure</p>
</li>
<li>
<p>Slab allocator</p>
<ul>
<li>Memcached Items
<ul>
<li>Item that holds data for a k,v pair</li>
</ul>
</li>
<li><a href="/posts/allocators/">allocators</a></li>
<li>A slab is a list of memory sections containing objects (pages) categories by the memroize size range they fall into
<ul>
<li>By default the memory is split into 42 slabs</li>
<li>first slab for items less than 96 bytes</li>
<li>second for 96-120 bytes</li>
<li>etc</li>
</ul>
</li>
</ul>
</li>
<li>
<p>LRU list</p>
<ul>
<li>For evicting the oldest and least used items, LRU matches to different slabs</li>
</ul>
</li>
</ul>
<h3 id="memcache-design">Memcache Design</h3>
<ul>
<li>Adaptive slab allocator
<ul>
<li>Peroidic rebalancing of slabs</li>
<li>If slabs needs more memory if it is evicting a lot of items, or if an item that is evicted was being used 20% more recently than the average of the least recently used item in other slab classes</li>
<li>When we identify a needy slab, we free up the least recently used slab and transer it to the needy class</li>
</ul>
</li>
<li>Short lived keys are pruned using a circular buffer than removes one item every second</li>
</ul>
<h3 id="memcache-clustering">Memcache Clustering</h3>
<ul>
<li>Key load is managed by consistent hashing</li>
<li>although this means we need a memcached router for balancing, hence <a href="https://github.com/facebook/mcrouter">https://github.com/facebook/mcrouter</a></li>
<li>MC router talks with the memcache ASCII protocol</li>
<li>Network efficiency issue: incast congestion
<ul>
<li>When a large number of responses to request overload a DC&rsquo;s cluster and rack switches, when we to rack to rack, such as broadcasting with memcache</li>
<li>Use a sliding window, similar to TCP</li>
</ul>
</li>
<li>High load issues
<ul>
<li>Stale sets
<ul>
<li>When a sequence of concurrent operations get reordered</li>
</ul>
</li>
<li>Thundering herd
<ul>
<li>When multiple clients attempt to request the same thing at once</li>
</ul>
</li>
<li>Solution: use leases
<ul>
<li>We &ldquo;lease&rdquo; the key value to a client by allocating a token, to set the item, the client has to provide the token again</li>
<li>A token can be invalidated if the Memcached server receives a delete request for the key
<ul>
<li>Similar to load-linked and store-conditionals</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Different applications need different sizes, what do we do?
<ul>
<li>Different pools of memcached, such as low-churn keys and high-churn keys</li>
<li>Replication within pools to improve latency and divide the load
<ul>
<li>Sometimes we want to split the keyset, but other times we might just want to replicate the key set</li>
</ul>
</li>
</ul>
</li>
<li>Small outages
<ul>
<li>Add &ldquo;gutter&rdquo; servers, which do not rehash the key-value items to other Memcached using consistent hashing. K/V&rsquo;s in the gutter pool are drained quickly</li>
</ul>
</li>
</ul>
<h3 id="regional-level-memcache">Regional Level Memcache</h3>
<ul>
<li>Scaling a cluster natively makes it so that our networks start to face incast congestion</li>
<li>We&rsquo;d rather replicate clusters</li>
<li>But how do we invalidate when we replicate?</li>
<li>THere needs to be two ways of invalidation: either the backend DB invalidates the key on an update, or the client does
<ul>
<li>We can set an invalidation daemon that discovers or just broadcasts UDP to all memcached instances to invalidate</li>
<li>Note that the dameons batch as well</li>
</ul>
</li>
</ul>
<h3 id="controlling-replication-in-regional-pools">Controlling replication in regional pools</h3>
<ul>
<li>How many places should keys be replicated?</li>
<li>Create a regional pool of memcached servers that multiple clients share, and move data between different pools as needed</li>
</ul>
<h3 id="bringing-up-new-pools">Bringing up new pools</h3>
<ul>
<li>New pools should look at warm pools to replicate, not the DB, otherwise it threatens to overwhelm the DB</li>
<li>To avoid races, we hold off <code>delete</code> operations, and clients add to the new cluster instead of the old</li>
</ul>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/2021_goals/">2021 goals</a></li>
       
          <li><a href="/posts/actor_model/">actor model</a></li>
       
          <li><a href="/posts/broadcasting_and_multi_cast/">broadcasting and multicast</a></li>
       
          <li><a href="/posts/bully_algorithm/">bully algorithm</a></li>
       
          <li><a href="/posts/byzantine_fault_tolerance_bft/">byzantine fault tolerance (BFT)</a></li>
       
          <li><a href="/posts/cap_theorm/">cap theorm (brewer&#39;s conjecture and the feasibility of consistent, available, partition-tolerant web services</a></li>
       
          <li><a href="/posts/20200415152613-cassandra/">Cassandra</a></li>
       
          <li><a href="/posts/clocks_distributed_systems/">clocks (distributed systems)</a></li>
       
          <li><a href="/posts/crdts/">crdts</a></li>
       
          <li><a href="/posts/dataflow/">dataflow</a></li>
       
          <li><a href="/posts/distributed_systems_for_fun_and_profit/">distributed systems for fun and profit</a></li>
       
          <li><a href="/posts/distributed_transactions/">distributed transactions</a></li>
       
          <li><a href="/posts/epaxos/">epaxos</a></li>
       
          <li><a href="/posts/gossip_protocols/">gossip protocols</a></li>
       
          <li><a href="/posts/gray_failures/">gray failures</a></li>
       
          <li><a href="/posts/homogenous_time_blog_post/">Homogenous Time: Ibn Khaldun and Transaction Logs</a></li>
       
          <li><a href="/posts/impossibility_of_consensus_with_one_faulty_process/">impossibility of consensus with one faulty process</a></li>
       
          <li><a href="/posts/kubernentes/">kubernetes (k8s)</a></li>
       
          <li><a href="/posts/load_balancing/">load balancing</a></li>
       
          <li><a href="/posts/metastable_failures_blog_post/">metastable failures blog post</a></li>
       
          <li><a href="/posts/paxos/">paxos</a></li>
       
          <li><a href="/posts/physalia_amazon_ebs/">physalia - amazon ebs</a></li>
       
          <li><a href="/posts/raft/">raft</a></li>
       
          <li><a href="/posts/windows_failover_clusters_hci/">windows failover clusters (hci)</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>
</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
