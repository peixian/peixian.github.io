<!DOCTYPE html>
<html><title>paxos</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.532078bb650bec3433e8e9db19e0453faa66c9cc0ce761054558a91d87039e9d.css"/>
<script defer src="/en.search.min.c904e5dcf1ec4e9dd81685dfe7b3e898d2725960e889f708ebfcb11e7c863e87.js" integrity="sha256-yQTl3PHsTp3YFoXf57PomNJyWWDoifcI6/yxHnyGPoc="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>paxos</h1>

          <p><a href="/posts/algorithms/">algorithms</a>, <a href="/posts/distributed_systems/">distributed systems</a></p>
<ul>
<li>FLP impossiblity result means that even if one crash is tolerated in an async system, consensus cannot be reached</li>
</ul>
<h2 id="tradeoffs-table">Tradeoffs Table</h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Leader Election RTTs</th>
<th>Voting RTTs (General Case)</th>
<th>Voting RTTs (Optimistic Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fast Paxos</td>
<td>n/a (leaderless)</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td><a href="/posts/multi_paxos/">multi-paxos</a></td>
<td>1</td>
<td>2</td>
<td>n/a</td>
</tr>
<tr>
<td><a href="/posts/epaxos/">epaxos</a></td>
<td>n/a (leaderless)</td>
<td>2</td>
<td>n/a</td>
</tr>
<tr>
<td><a href="/posts/raft/">raft</a></td>
<td>1</td>
<td>2</td>
<td>n/a</td>
</tr>
</tbody>
</table>
<h2 id="implementation-of-paxos">Implementation of Paxos</h2>
<ul>
<li>Paxos is generally used to keep track of a log, and decide which values are appropriate to commit to a log</li>
<li>Divded into proposers, acceptors, and learners
<ul>
<li>Proposers send their proposals to other replica group members, and tries to have a majority of members support their proposal.</li>
<li>Acceptors are the group members who receive proposals or reject them</li>
<li>Learners keep track of accepted values, and mark them as chosen. Learners are basically voters that help consensus</li>
</ul>
</li>
</ul>
<h3 id="steps">Steps</h3>
<ul>
<li>Two steps, first a majority voting step and and commit steps.</li>
<li>The reason there&rsquo;s two steps is because of the potential for a split vote, where the acceptor only accepts the first value, and no majority is chosen
<ul>
<li>Conflicting choices can also happen and result in ABA</li>
<li>However, two phases is not enough to enforce safety, we must have additional members</li>
</ul>
</li>
<li>Proposal numbers play a vital role, acting as logical clocks.</li>
</ul>
<p>Actual steps:</p>
<ol>
<li>Handshake phase - prepare-promise message, a prepare request is sent by each proposer to all acceptors. <code>prepare(proposal_number)</code> is called, everyone either accepts or rejects the proposal
<ol>
<li>If the acceptor hasn&rsquo;t yet promised to any other prepare request, it records the proposal number and sends a promise to the proposer</li>
<li>If the acceptor also already promised to a pepare request with lower proposal number,  it makes a promise with the newer one and breaks the promise with the already promised request.</li>
<li>If the acceptor has already promised to a prepare request with the higher numbered proposal, it simply requests the current one</li>
<li>If the acceptor has already accepted an accept request with a lower proposal number, it makes a promise to the current rprepare, and sends back the accepted proposal number and the corresponding value</li>
</ol>
</li>
<li>Value acceptance phase - accept-accepted messages
<ul>
<li>If the proposer receieves a promise from the majority of acceptors, it sends an accept request <code>accept(proposal_number, value)</code></li>
<li>This includes the value so that
<ol>
<li>If the proposer didn&rsquo;t recieve any accepted proposals along with the promise message in response to the prepare requests, then it will propose a value of its choice</li>
<li>If the proposer received already accepted proposals by the acceptors, along with the promise messages in response to the prepare requests, then it has to select the value with the highest proposal number</li>
</ol>
</li>
<li>An acceptor, on recieving an accept request, responds to the proposer
<ol>
<li>If the latest proposal number promised by the acceptor is the same as in the accept request, it will accept the request and respond to the proposer with an accepted message. If the proposer recieves majority acceptance of the proposed value, the value is chosen and broadcast to everyone</li>
<li>If the acceptor has promised to a new proposal with the higher proposal number, it rejects the accept request, and the protol has to start again</li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="liveness-issues">Liveness Issues</h3>
<ul>
<li>Since an acceptor always promises on a proposal, it can lead to a situatiion where accept rejects sent by a proposer are always being rejected by the acceptors if there is lag.</li>
<li>If every time the acceptors recieve a prepare request with a higher proposal number before receiving an accept requests against a proposal that the acceptors promised earlier, none of the accept requests will be accepted.</li>
<li>To solve this, only single proposers are allowed to issue proposals (aka leaders)</li>
</ul>
<h3 id="other-considerations">Other Considerations</h3>
<ul>
<li>Why majority instead of highest number? Could result in ties</li>
<li>Why two steps? Having a majority always choose a value is bad because then you could get overmatching writes</li>
</ul>
<h3 id="multipaxos">Multipaxos</h3>
<ul>
<li>Basically between paxos rounds, if you have the same leader, you can just just propose and commit, since you know there&rsquo;s no competing proposer</li>
</ul>
<h2 id="boxwood">Boxwood</h2>
<ul>
<li>Old system used to explore the feasibility of various stages in DB locks
<ul>
<li><a href="https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/maccormick/maccormick_html/index-1.html">https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/maccormick/maccormick_html/index-1.html</a></li>
</ul>
</li>
</ul>
<h2 id="chubby">Chubby</h2>
<ul>
<li>Google locking system that predates <a href="#zookeeper">Zookeeper</a></li>
<li>Used by <a href="/posts/storage/#gfs--google-file-system">GFS (Google File System)</a> to appoint a manager server and stores small amounts of data</li>
<li>Used by <a href="/posts/databases/#bigtable">BigTable</a> to elect bigtable amanger, discover servers, enable clients to locate Bigtable manager, and low volume storage</li>
</ul>
<h3 id="requirements">Requirements</h3>
<ul>
<li>Provides 3 APIs: whole file reads and writes, advisory locks, and notification of events</li>
<li>Coarse grained locking service, you can lock with minimal overhead, and is a reliable low volume storage</li>
<li>Also needs to be available and reliable, and have decent throughput</li>
</ul>
<h3 id="design">Design</h3>
<figure><img src="/ox-hugo/2024-03-02_20-38-16_screenshot.png"/>
</figure>

<h4 id="chubby-cell">Chubby Cell</h4>
<ul>
<li>Chubby cell is multiple serbers (usually 5), which replicate with each other</li>
<li>Each server has a namespace composed of directions and files, with ACLs</li>
<li>One replica is always elected as the primary, which initiates read and write operations</li>
<li>Replicas copy the database using a consensus protocol and elect new primaries as needed</li>
<li>Clients discover the primary by asking any server within the cell which is the primary, and caches it on the client side. It uses this as the primary until the server stops being the primary</li>
<li>Two types of requests
<ul>
<li>Writes: propagated to all the replica servers, are async and only acked when a quorum responds</li>
<li>Reads: serviced by the primary replica</li>
</ul>
</li>
<li>Each file or directory within a chubby server is knownas a <code>node</code> (similar to a <a href="/posts/inodes/">inodes</a>)
<ul>
<li>Each node has a unique name, there&rsquo;s emphemeral and permanent nodes</li>
<li>Ephemeral nodes are similar to temp files, they get deleted when no client has them open</li>
</ul>
</li>
<li>Uses ACLs in the directory that keeps track of authorized names</li>
</ul>
<h4 id="chubby-library">Chubby Library</h4>
<ul>
<li>Each client communicates with chubby cells via the client</li>
<li>Keeps track of the primary replicas to communicate with</li>
</ul>
<h3 id="locking">Locking</h3>
<ul>
<li>multiple clients can hold a lock in reader mode, only a single client can hold a writer lock</li>
<li>Locks are adivsory, which means you don&rsquo;t need a lock to read a file. Clients are required to cooperate for conflicts, but advisory locks are more scalable rather than mandatory locks, and clients can emulate strict locking easily (just have them explicitly check for locks)</li>
<li>If clients are holding a read lock, a write lock cannot be acquired</li>
</ul>
<h4 id="sequencers-and-lock-delays">Sequencers and lock delays</h4>
<ul>
<li>If you hold a lock and never release it, then we have issues</li>
<li>Sequence numbers are introduced after a lock is acquired</li>
<li>Locks that are not released are expired, and we use sequence numbers to note the current state (similar to a lamport clock)</li>
<li>If a lock becomes free after an expiration, the lock server does not let any other client claim it for a specific time period. This is used to create a backoff from faulty clients claiming a lock, then releasing, then claiming it again.</li>
<li>To allow systems to know what is happening, Chubby sends events: modifying file contents, modifying nodes, replica failover, handles, locks, conflicts, etc</li>
</ul>
<h3 id="caching">Caching</h3>
<ul>
<li>Primaries keeps a list of data that clients are caching and sends invalidations to the clients to keep them consistent</li>
<li>If data or metadata needs to be changed, the primary blocks modifications and sends invalidations to clients with the relevant data cached</li>
<li>When a client receives an invalidation, it flushes the invalid state and acks it with a keepalive
<ul>
<li>If there&rsquo;s no acks for invalidations, then the primary keeps the node uncachable</li>
</ul>
</li>
</ul>
<h3 id="sessions">Sessions</h3>
<ul>
<li>A client and a cell keep track of each other with sessions that are held with keepalives</li>
<li>A session comes with a lease, which is defined as a time period where the primary will tell the client with updates and will not terminate the connection unilaterally</li>
<li>The lease is used by the local client to know if something has gone wrong, if it&rsquo;s missing keepalives
<ul>
<li>If a local lease times out, then the client
<ul>
<li>Empties and disables the cache</li>
<li>Waits for a grace period, and tries keepalives</li>
<li>If it connects it, then enable the cache</li>
<li>If nothing, then assume terminated</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="failovers">Failovers</h3>
<ul>
<li>If a node cannot communicate with the primary after the lease ends, it starts and election</li>
<li>Periodically, the primaries keep a keepalive for the leadership</li>
<li>Each cell&rsquo;s primary takes snapshots and backs it up</li>
<li>Mirroring can also occur across different regions, and a mirror that cannot be accessed remains unaltered until communication is reestablished. Updated files are located by contrasting their checksums</li>
<li>Failure steps for replicas that do not recover after a few hours
<ul>
<li>A replacement system is used to provision a new replica</li>
<li>It initaties the lock server binary</li>
<li>DNS tables are updated</li>
<li>Current primary replica also has to have this info, which it polls for</li>
<li>Cell DB is updated</li>
<li>Replica servers update themselves with the new member</li>
<li>New server recieves information</li>
<li>Afterwards, it is permitted to vote in an election after it processes a write</li>
</ul>
</li>
</ul>
<h3 id="proxies-and-partitioning">Proxies and Partitioning</h3>
<ul>
<li>Proxies act as LB&rsquo;s by allowing KeepAlives to be decreased to the main server</li>
<li>Partitioning allows us to shard chubby&rsquo;s namespace, where different namespaces set different replicas</li>
</ul>
<h3 id="design-decisions">Design Decisions</h3>
<ul>
<li>Use a lock service for centrally managed things</li>
<li>Permit a huge number of clients to access a Chubby file without using many servers</li>
<li>Use event notification instead of polling because clients and replicas may want to know when things have changed</li>
<li>Cache the data on the client side</li>
<li>Use consistent caching because developers get confused by non-intuitive caching semantics</li>
<li>Redirecting all reads and writes via a single node was the way to provide strong consistentcy. Caches and proxies make up for the loss in R/W throughput</li>
</ul>
<h4 id="differences-from-boxwood">Differences from Boxwood</h4>
<ul>
<li>Chubby&rsquo;s aspects (lock system, small file storage, and session/lease management) are one thing, whereas boxwood had three different peices</li>
<li>Chubby has a more advanced interface than boxwood</li>
<li>Boxwood had a 200ms lease period, whereas Chubby&rsquo;s is 12</li>
<li>Chubby has a grace period to prevent losing locks</li>
</ul>
<h3 id="coarse-grained-vs-fine-grained-locking">Coarse grained vs fine-grained locking</h3>
<ul>
<li>Coarse grained locks will need much less load on the lock server, they are rarely acquired</li>
<li>However, transferring locks from one client to another needs expensive recovery procedures</li>
<li>Fine grained locks are frequently accessed, which means availability would become critical</li>
<li>Time penalty for dropping locks would not be severe, since locks are only held for a short period</li>
</ul>
<h2 id="zookeeper">Zookeeper</h2>
<h3 id="api">API</h3>
<ul>
<li><code>create(path, data[], mode, flag)</code>
<ul>
<li>Creates a znode, like an inode, but a unit of abstraction to lock on it</li>
</ul>
</li>
<li><code>setData(path, data[], version)</code>
<ul>
<li>sets the data[] in the znode at the specified flag</li>
</ul>
</li>
<li><code>getData(path, watch)</code>
<ul>
<li>returns <code>data[]</code>, also allows clients to watch</li>
</ul>
</li>
<li><code>getChildren(path, watch)</code>
<ul>
<li>Returns all the children names of the znode at the specified path</li>
</ul>
</li>
<li><code>exists(path, watch)</code>
<ul>
<li>Checks whether a znode exists</li>
</ul>
</li>
<li><code>delete(path, version)</code>
<ul>
<li>Deletes a znode. Must match the monotonically increasing versions</li>
</ul>
</li>
</ul>
<h3 id="server">Server</h3>
<ul>
<li>Fully replicated, like <a href="/posts/etcd/">etcd</a>, into a set called a <code>ZooKeeper ensemble</code></li>
<li>Elected leaders and others become followers</li>
<li>Unlike chubby, clients can connect to any
<ul>
<li>Higher availablity, not as strong consistency</li>
<li>Use <code>sync()</code> to assure that you have the highest consistency</li>
</ul>
</li>
<li>Leader broadcasts operations to the followers, and performs write operation on the coordination data placed in its memory</li>
<li>Follower can also recieve and respond to write requests. Multiple writes can be batched, but only the request needs to forwarded to the leader, the leader broadcasts all the requests to other followers
<ul>
<li>Basically a follower gets a write, sends a request to the leader, the leader broadcasts the state to everyone after it&rsquo;s done</li>
</ul>
</li>
</ul>
<h3 id="replicated-database">Replicated Database</h3>
<ul>
<li>In memory copy of the database so that reads and writes can be done locally</li>
<li>ZK takes periodic snapshots of all the delivered messages as a WAL
<ul>
<li>Snapshots are fuzzy</li>
<li>Enables at-most-once execution</li>
<li>If a server dies before the next snapshot is taken, it does a depth first scan of the tree to read every znode&rsquo;s metadata and data atomically then write that metadata and data from disk, extracted from the WAL
<ul>
<li>This is why ZK recovery is <strong>slow</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="atomic-broadcast">Atomic Broadcast</h3>
<ul>
<li>Used by ZK to broadcast the write request to the replicated database on all servers
<ul>
<li>Followers forward writes to leaders</li>
</ul>
</li>
<li>Uses the ZAB protocol
<ul>
<li>Two modes
<ol>
<li>Broadcast is used to send messages</li>
<li>Recovery is used to syncronize</li>
</ol>
</li>
<li>Default is \(2f + 1\) for quorum where f is the number of faults</li>
</ul>
</li>
<li>When a leader dies, the new leader is elected and ensures all the updates from the previous leader are incorporated into its replicated db before it broadcasts its own requests</li>
<li>At every transaction, the leader is broadcasting what it is working on into a replicated queue</li>
</ul>
<h3 id="request-processor">Request Processor</h3>
<ul>
<li>Manager that keeps the transactions atomic, only the leader uses this</li>
<li>Transactions within ZK are idempotent</li>
<li>All requests are linearized on the leader, so the leader converts everything into a <code>setDataTXN</code></li>
</ul>
<h3 id="client-server-interactino">Client-server interactino</h3>
<ul>
<li><code>getData()</code> and <code>getChildren()</code> can both be performed locally without notifying others</li>
<li>Servers generate <code>zxid</code> for every read request and retrieves the most recently updated state of the server&rsquo;s data</li>
<li>Writes have multistep
<ol>
<li>Write the data</li>
<li>Notify the client(s) who have set watches on that data</li>
<li>Sent the write request to the leader so it can be updated in the replicated database</li>
<li>Data is replicated among all connected servers in the ensemble</li>
</ol>
</li>
<li>Everything is FIFO, including read requests when there&rsquo;s a write</li>
<li>However if a read is in progress, multiple readers can read in parallel</li>
<li>Like a giant R/W lock</li>
<li>Default is async transmission of data, which comes at the cost of insconsistent or stale reads</li>
<li>the <code>zxid</code> allows the client to maintain a consistent view across different servers</li>
<li>Session timeouts and heartbeats are used to maintain connections from servers to clients for watches, which is decided by the leader</li>
</ul>
<h3 id="primitives">Primitives</h3>
<ul>
<li>
<p><a href="https://zookeeper.apache.org/doc/r3.4.2/recipes.pdf">https://zookeeper.apache.org/doc/r3.4.2/recipes.pdf</a></p>
</li>
<li>
<p>Config management and service discovery</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(&#34;/config/port&#34;, 8090, EMPHEMERAL) # set a port
</span></span><span style="display:flex;"><span>getData(&#34;/config/port&#34;, true) # get a watch
</span></span><span style="display:flex;"><span>setData(&#34;/config/port&#34;, value, version) # new version
</span></span></code></pre></div></li>
<li>
<p>Rendezvous</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(&#34;/rendezvous/candidateOne&#34;, &#34;&#34;, EMPHEMERAL) # create a znode to write
</span></span><span style="display:flex;"><span>getData(&#34;/rendezvous/candidateOne&#34;, true) # get a watch
</span></span><span style="display:flex;"><span>setData(&#34;/rendevous/candidateOne&#34;, {&#34;10.0.0.1&#34;, 8080})
</span></span></code></pre></div></li>
<li>
<p>Group Membership</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(&#34;/groups/member0001&#34;, EMPHEMERAL)
</span></span><span style="display:flex;"><span>newZnode = create(&#34;/groups/member0001/processOne&#34;, EMPHERAL,SEQUENTIAL) # set a sequential flag to generate a unique name
</span></span><span style="display:flex;"><span>childrenList = getChildren(/groups/member0001&#34;, true)
</span></span></code></pre></div></li>
<li>
<p>Locks</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newLock = create(&#34;/locks/lock-1&#34;, EMPHEMERAL)
</span></span><span style="display:flex;"><span>exists(&#34;/locks&#34;, true) # watch the lock tree
</span></span><span style="display:flex;"><span>delete(newLock) # to release the lock
</span></span></code></pre></div><ul>
<li>issues
<ul>
<li>
<p>herding: when a lock is released, many clients stampede to get the lock</p>
<ul>
<li>Solved with using <code>SEQUENTIAL</code>, at which point the lowest sequence number will hold the lock
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(path + &#34;/lock-&#34;, EPHEMERAL, SEQUENTIAL)
</span></span><span style="display:flex;"><span>do
</span></span><span style="display:flex;"><span>    childrenList = getChildren(path, false)
</span></span><span style="display:flex;"><span>    if newZnode is lowest znode in children
</span></span><span style="display:flex;"><span>        exit
</span></span><span style="display:flex;"><span>    newPath = znode in childrenList ordered just before newZnode
</span></span><span style="display:flex;"><span>    if exists(newPath, true)
</span></span><span style="display:flex;"><span>        wait for event
</span></span><span style="display:flex;"><span>while true
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>exclusive lock only</p>
<ul>
<li>Can implement R/W locks with
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(path + &#34;/write-&#34;, EPHEMERAL, SEQUENTIAL)
</span></span><span style="display:flex;"><span>do
</span></span><span style="display:flex;"><span>    childrenList = getChildren(path, false)
</span></span><span style="display:flex;"><span>    if newZnode is lowest znode in children
</span></span><span style="display:flex;"><span>        exit
</span></span><span style="display:flex;"><span>    newPath = znode in childrenList ordered just before newZnode
</span></span><span style="display:flex;"><span>    if exists(newPath, true)
</span></span><span style="display:flex;"><span>        wait for event
</span></span><span style="display:flex;"><span>while true
</span></span></code></pre></div></li>
<li>create a read lock and then reads can happen with other reads, but blocks on writes, and writes block on reads</li>
</ul>
</li>
</ul>
</li>
<li>Barriers
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>newZnode = create(path + &#34;/&#34; + processName)
</span></span><span style="display:flex;"><span>exists(path + &#34;/ready&#34;, true)
</span></span><span style="display:flex;"><span>newZnodeChild = create(newZnode, EPHEMERAL)
</span></span><span style="display:flex;"><span>childrenList = getChildren(path, false)
</span></span><span style="display:flex;"><span>if fewer children in childrenList than barrierThreshold
</span></span><span style="display:flex;"><span>  wait for watch event
</span></span><span style="display:flex;"><span>else
</span></span><span style="display:flex;"><span>  create(path + &#34;/ready&#34;, REGULAR)
</span></span></code></pre></div></li>
</ul>
</li>
</ul>
<h3 id="performance">Performance</h3>
<ul>
<li>
<p>Better load distribution than chubby</p>
<figure><img src="/ox-hugo/2024-03-04_20-09-16_screenshot.png"/>
    </figure>

</li>
<li>
<p>Atomic broadcast decreases performance, has a CPU cost</p>
</li>
<li>
<p>Failures of leaders will grind application to zero while it recovers</p>
</li>
<li>
<p>Solves chubby like problems with different tradeoffs, gives up consistency for a bit of throughput</p>
</li>
</ul>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/databases/">Databases</a></li>
       
          <li><a href="/posts/distributed_systems/">distributed systems</a></li>
       
          <li><a href="/posts/epaxos/">epaxos</a></li>
       
          <li><a href="/posts/k_v_stores/">k/v stores</a></li>
       
          <li><a href="/posts/kafka/">kafka</a></li>
       
          <li><a href="/posts/moraru_et_al_there_is_more_consensus_in_egalitarian_parliaments/">Moraru et al: There Is More Consensus in Egalitarian Parliaments</a></li>
       
          <li><a href="/posts/multi_paxos/">multi-paxos</a></li>
       
          <li><a href="/posts/paxos_vs_raft_reading_group/">paxos vs raft reading group</a></li>
       
          <li><a href="/posts/raft/">raft</a></li>
       
          <li><a href="/posts/wang_et_al_on_the_parallels_between_paxos_and_raft_and_how_to_port_optimizations/">Wang et al: On the Parallels between Paxos and Raft, and how to Port Optimizations</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>
</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
