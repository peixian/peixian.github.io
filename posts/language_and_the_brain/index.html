<!DOCTYPE html>
<html><title>Language and the Brain (class)</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.7e9d163bca6e3791417a0e81190cd6067c7d0412819ec9e96db3dc3f5e88ce8c.css"/>
<script defer src="/en.search.min.e6f5605c0d126afb10fbff35a09dd12df8f9e5ef19cd32466adf2c741b0bd464.js" integrity="sha256-5vVgXA0SavsQ&#43;/81oJ3RLfj55e8ZzTJGat8sdBsL1GQ="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>Language and the Brain (class)</h1>

          <p><a href="/posts/fall_2021_classes/">fall 2021 classes</a></p>
<h2 id="class-1">Class 1</h2>
<ul>
<li>What do linguists do?</li>
<li>Linguists as part of cognitive science
<ul>
<li>What is the consquences of this view?</li>
<li><a href="/posts/chomsky_syntatic_structures/">Chomsky - Syntatic Structures</a> as the dawn of generative grammar</li>
</ul>
</li>
<li>There exists some sentences of setnences such that it allows for recursive composition</li>
<li>So once we setup the idea that English cannot be a FSM, what is the data?</li>
<li>Langauges that use case markings
<ul>
<li>case markings allow it to mtaintain a more consistent structure modulo case marking</li>
</ul>
</li>
<li>chomsky is unconcerned with the statistical metadata that arises from a corpus
<ul>
<li>&ldquo;internalist&rdquo; - grammars and structures within a single sentence</li>
<li>&ldquo;externalist&rdquo; - set of sentences within a corpus</li>
</ul>
</li>
<li>for sentences to make sense, we assign a phoronlogical structure to pronounce sentences
<ul>
<li>which then means a sentence makes sense iff:
<ul>
<li>syntax (chomsky is here) is correct</li>
<li>phoronological sufficent (this can be derived from statistics)</li>
</ul>
</li>
</ul>
</li>
<li>&ldquo;there exists a faculty of language&rdquo; - but is there a <!-- raw HTML omitted -->single<!-- raw HTML omitted --> faculty of langauge</li>
</ul>
<h2 id="class-2-9-13">Class 2 - 9/13</h2>
<ul>
<li>what is the neo-durkheimian relationship between langauge and thought?</li>
<li>is language just a set of sentences?
<ul>
<li>in the 70&rsquo;s, they thought that sentences are a sequence of words or sounds</li>
<li>what does this leave out? -&gt; hidden syntax and semantics
<ul>
<li>syntax cannot directly be seen</li>
<li>semantics sometimes contain a truth value</li>
<li>oftentimes this forms the \(\{form/sound, meaning\}\)</li>
<li>chomsky identified this as a triplet of \(\{form, syntax, meaning\}\)</li>
</ul>
</li>
</ul>
</li>
<li>sentences in english are infinite due to recursion (aka not finite structured)_
<ul>
<li>chomsky also distinguished between &ldquo;lists&rdquo; and &ldquo;conjunctions&rdquo;</li>
<li>lists as \(\mathbb{N}\)</li>
<li>however, langauge (or at least english) is &ldquo;uninterestingly&rdquo; infinite
<ul>
<li>as you push further out and generate more sentences, the sentences themselves become regularized</li>
</ul>
</li>
</ul>
</li>
<li>for chomsky, grammar is the compression of language, and if you memorize words you can generate an infinite set of sentences</li>
<li>also must make a distinction between &ldquo;actual&rdquo; and &ldquo;potential&rdquo;
<ul>
<li>&ldquo;actual&rdquo; sentences are bound by the maximum speaker time or computation time</li>
<li>&ldquo;potential&rdquo; sentences are the infinite set of all sentences generatable</li>
</ul>
</li>
<li>what are the grammatical rules for telling us if a word is an actual word?
<ul>
<li>&ldquo;moak&rdquo; -&gt; is this a word?</li>
<li>we have grammars to say this is a potential word, but to reject a word, we actually do a semi-exhaustive search through our internal dictionaries to find out</li>
</ul>
</li>
<li>some words enter the vernacular after it is used
<ul>
<li>&ldquo;jabberwocky&rdquo;, for example</li>
</ul>
</li>
<li>how do we distinguish words that are archaeic/low frequency from words that are incorrect?
<ul>
<li>side note <a href="/posts/finite_state_languages/">finite state languages</a> can delinate infinite sets</li>
<li>this is known as the &ldquo;<a href="/posts/strong_and_weak_generative_capacity/">strong and weak generative capacity</a>&rdquo;</li>
</ul>
</li>
<li>what was the theory of cognitive science before the 60&rsquo;s?
<ul>
<li>behavioralism</li>
</ul>
</li>
<li><!-- raw HTML omitted -->learning is the intentional modification of the environment in order to produce different future behaviors<!-- raw HTML omitted --></li>
<li>what is actually being &ldquo;learned&rdquo; when you acquire a langauge?
<ul>
<li>a child does not do a blind word/object association (i.e. this is a ball)</li>
<li>the faculty of langauge extends beyond this into &ldquo;teaching&rdquo; or &ldquo;learning&rdquo; a grammar and a generative capacity</li>
</ul>
</li>
<li>grammars have variance built in
<ul>
<li>this is why we speak differently</li>
<li><a href="/posts/language_accomodation_in_arabic/">language accomodation in arabic</a></li>
</ul>
</li>
</ul>
<h2 id="class-3-9-15">Class 3 - 9/15</h2>
<ul>
<li>Mendel&rsquo;s laws - laws of inheritances that require abstract concepts like genes
<ul>
<li>organized in such a way that gives rise to laws</li>
</ul>
</li>
<li>linguistic laws
<ul>
<li>grimm&rsquo;s law - stated in terms of independent sounds - phonemes</li>
<li>werner&rsquo;s law</li>
<li>before these system laws, the idea that sound changes happened consistently/completely across langauges</li>
</ul>
</li>
<li>when chomsky says linguistics as a cognitive science
<ul>
<li>this is not to reject the generalization finding tools</li>
</ul>
</li>
<li>dan everett - linguist in piraha
<ul>
<li>no recursion in this language</li>
<li>he proposes it comes from culture</li>
</ul>
</li>
<li>many things are understood about linguistics via numbers and color systems</li>
<li>how to understand rules?
<ul>
<li>through the enforcement of cultural roles
<ul>
<li>how are the roles communicated and enforced?</li>
<li>what happens if you violate it?
<ul>
<li>shades of similarity to <a href="/posts/hart_s_theory_of_law/">hart&rsquo;s theory of law</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>studies about generalizablity of specific groups have never been totally innocent - largely driven by social policy</li>
<li>does grimm&rsquo;s law focus on the right things?
<ul>
<li>are phonemes the correct unit of generalization?</li>
<li>the persuasion of a generalization lies in the correlation of its properties</li>
</ul>
</li>
<li>future is tied up with the concept of &ldquo;irrealis&rdquo;
<ul>
<li>modals, conditions</li>
<li>future</li>
<li>counterfactuals</li>
</ul>
</li>
<li>future tense does not seem to be a a fundalmental building blocks of language</li>
<li>present tense is a mess because stateive and others</li>
<li>grimm&rsquo;s laws are simple, but the explanation could be complicated</li>
<li>&ldquo;laws&rdquo; of languages often end up generalizing idiosyncratic effects to that languge</li>
<li>anthropoligists suggest that systematic study of langauge by pieces that exhibit discontinuities reflect culture
<ul>
<li>is culture downstream of langauge?</li>
</ul>
</li>
<li>compositionality
<ul>
<li>meaning is composition</li>
<li>labels can be generated from the hash of meaning</li>
<li>label is the form/shape/demarker of the word?</li>
</ul>
</li>
</ul>
<h2 id="class-5-9-22">Class 5 - 9/22</h2>
<ul>
<li>Is phornology finite state?
<ul>
<li>is it less powerful than finite state?</li>
</ul>
</li>
<li>why do we care where in the brain something happens
<ul>
<li>it matters how the brain recongizes faces, but to do so, we must first learn where</li>
</ul>
</li>
<li>jakobson
<ul>
<li>invariance</li>
<li><code>/t/</code> - phoneme, slashes denote an obstruct t</li>
<li><code>/t/</code> - goes into \([t^h]\) (as in &ldquo;top&rdquo;) and \([t]\) (as in &ldquo;stop&rdquo;, this doesn&rsquo;t actually exist in mandarin)</li>
</ul>
</li>
<li>the other inviariance is when you have two different phonemmes, but how do we know what the nominative case is?</li>
<li>Jakobson says it is meaning
<ul>
<li>the features are semantic ones</li>
<li>therefore, it was incorrect to think of this as the level of the phoneme, but rather we should look at it from the meaning</li>
</ul>
</li>
<li>internal/eternal
<ul>
<li>prinicple of contrast
<ul>
<li>example: &ldquo;top&rdquo; vs &ldquo;(t^)hop&rdquo; - your pronouncation of &ldquo;t&rdquo; does not change the menaining, therefore the asperation is not used to define meaning</li>
</ul>
</li>
<li>feature grounding - in terms of articulation, Jakobson thought we should use acoustics (wrong)</li>
</ul>
</li>
<li>paper fights with sausser
<ul>
<li>meaning is distinct because it arises from the rest of the words
<ul>
<li>cat vs dog, elephant, etc</li>
</ul>
</li>
<li>sausser says language is ungrounded and that it arises from scarcity</li>
<li>it is because meaning can be generated through differences, like a hash function</li>
<li>what about <a href="/posts/religion/">religion</a>?
<ul>
<li>what does the invariance come from?</li>
</ul>
</li>
</ul>
</li>
<li>jakobson is still a version of structuralism</li>
<li>are word embeddings structuralist?</li>
<li>quine - early sentence embeddings -&gt; the context is what matters, the company it keeps</li>
<li>possiblies of derivational frequencies
<ul>
<li>derevational family entropy says there is neural reponse</li>
<li>why do generalizations happen in suffixes in english?</li>
</ul>
</li>
</ul>
<h2 id="class-6-9-27">Class 6 - 9/27</h2>
<ul>
<li>Standard linguistics metholdogy is the method of contrast principles
<ul>
<li>contrast structures that are parallels</li>
</ul>
</li>
<li>examples:</li>
</ul>
<table>
<thead>
<tr>
<th>Toler-</th>
<th>Teach-</th>
<th>Clash</th>
</tr>
</thead>
<tbody>
<tr>
<td>tolerant (adj)</td>
<td>teach (/)</td>
<td>Clash (noun)</td>
</tr>
<tr>
<td>tolerance (noun)</td>
<td>teach</td>
<td>Clash (verb)</td>
</tr>
<tr>
<td>tolerate (verb)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>tolerable (adj)</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>the (/) suffix is somethimes null, which contrasts with the existence sometimes
<ul>
<li>sometimes the metholodgy tells you the suffix is null</li>
</ul>
</li>
<li>when you read toler[ate|ance|able] your brain generates the frequency distribution
<ul>
<li>we have entropy/uncertainty over the continuities, called derivational family entropy</li>
</ul>
</li>
<li>derivational family entropy on mandarin?</li>
<li>example:
<ul>
<li>unflush(/) - no</li>
<li>unflushable - yes</li>
<li>why does (un) require a suffix?</li>
</ul>
</li>
<li>what does linguistics look like with implementation within the brain?</li>
<li>context free - a sentence consists of a noun phrase and a verb phrase</li>
<li>case markings - NP-&gt; NPACC/v-
<ul>
<li>when a noun phrase is accusative when its next to a verb, which are context sensitive</li>
</ul>
</li>
<li>A context free grammar cannot be duplicated. Lanugages like Yoruba which exhibits duplication, &ldquo;buildhouse-buildhouse&rdquo; -&gt; builder</li>
<li>Swiss german is the only clear examples of midly context-sensitive</li>
<li>Context-free is recursive (no memory)
<ul>
<li>for some languages, context sensitivy is used only for certain features, like reduplication in Yoruba
<ul>
<li>why is this a problem?
<ul>
<li>this points to an issue where human langauges have certain mathematical properties that map to the brain</li>
<li>langauges are not evenly distributed between context free &amp; context sensitive distribution</li>
<li>maybe we can&rsquo;t process certain context sensitivity</li>
</ul>
</li>
<li>finite -&gt; computablably enumerable are rough categories</li>
</ul>
</li>
</ul>
</li>
<li>pholonogy is theorized to be subregular</li>
<li>morpohology and syntax are much more complicated</li>
</ul>
<h2 id="class-7-9-29">Class 7 - 9/29</h2>
<ul>
<li>repetition implies context sensitivty
<ul>
<li>does english have similar properties?</li>
<li>np by np construction
<ul>
<li>&ldquo;dog after dog&rdquo; is different than &ldquo;little by little&rdquo;
<ul>
<li>little by little is not a noun phrase</li>
</ul>
</li>
<li>noun phrases are potentially infinite</li>
</ul>
</li>
</ul>
</li>
<li>heinz and isardi claim that phronology is sub-regular</li>
<li>constraints on grammar
<ul>
<li>generally deal with locality</li>
</ul>
</li>
<li>grammar trees can show you how things block locality</li>
<li>regular, natural, and CFG&rsquo;s circle layering</li>
<li>stabler
<ul>
<li>for each grammar of a specific type (MC) there eixists an efficient parser</li>
<li>stabler - subregularity of syntax gives you constraints on languages</li>
<li>some sentences cannot be extracted from subject and adjunct
<ul>
<li>called CED by J Huang</li>
<li>for langauges can do subextraction from subject</li>
<li>example: &ldquo;what did pictures of fall on Fred&rdquo; -&gt; &ldquo;what&rdquo; must be extracted from subject</li>
</ul>
</li>
</ul>
</li>
<li>merge ops
<ul>
<li>even if you limit ones to subregular, the merge operations does whatever you want
<ul>
<li>noun-phrase is equivilant to a noun-prep phrase, since a noun-phrase is a merge of noun-prep</li>
</ul>
</li>
</ul>
</li>
<li>let us assume subregular grammar math is the math used for human language -&gt; formal universal</li>
<li>langauges have nouns and verbs as invariants -&gt; Jakobson</li>
<li>Stabler sees that we parse strings of words: word by word
<ul>
<li>do you
<ul>
<li>generate sentences on your own until you find a match?
<ul>
<li>inefficient and non-deterministic</li>
</ul>
</li>
<li>since we do not wait until the end of a sentence before parsing it, model a sentence interpretation as a probability distribution that&rsquo;s incremental
<ul>
<li>lineraly porportional to the number of words</li>
<li>using the finite set of words and finite set of rules: the cross product of both gives you the search space</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li></li>
</ul>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/duara_the_crisis_of_global_modernity/">Duara - The Crisis of Global Modernity</a></li>
       
          <li><a href="/posts/fall_2021_classes/">fall 2021 classes</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>

</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
