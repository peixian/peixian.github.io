<!DOCTYPE html>
<html><title>scaling law seminar</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.4b9309a2ca0781957619d4e9500f6539479ebf8cdcbfaac702d6364b30e2c87c.css"/>
<script defer src="/en.search.min.8edcbf07840172083446117974cf9d6b10efbe4247ef9e68b3df1bc78c66ef5c.js" integrity="sha256-jty/B4QBcgg0RhF5dM&#43;daxDvvkJH755os98bx4xm71w="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>scaling law seminar</h1>

          <p><a href="/posts/fall_2021_classes/">fall 2021 classes</a>, <a href="/posts/transformers/">transformers</a></p>
<p>Seminar on scaling laws</p>
<h2 id="scaling-laws-for-acoustic-models">Scaling Laws for Acoustic Models</h2>
<ul>
<li>papers:
<ul>
<li><a href="https://arxiv.org/pdf/2106.09488.pdf">https://arxiv.org/pdf/2106.09488.pdf</a></li>
</ul>
</li>
<li>how do acoustic models scale with model size?</li>
<li>maybe ask haoran on this?</li>
<li><a href="https://assets.amazon.science/ef/a6/60f65ed543c9af2a519caba269bd/scaling-laws-for-acoustic-models.pdf">https://assets.amazon.science/ef/a6/60f65ed543c9af2a519caba269bd/scaling-laws-for-acoustic-models.pdf</a></li>
<li>what proposals are there? what does the current field look like for scaling laws?</li>
<li>most models use asr + LM&rsquo;s now, how does this scale with it overall?zz</li>
<li>does wav2vec scale? what are the profiles for wer in terms of longer speech datasets?</li>
</ul>
<h2 id="scaling-laws-for-solution-compressibility-aka-information-bottleneck--dot-dot-dot-dot-dropbox-org-information-theory-dot-md">Scaling laws for solution compressibility - aka <a href="/posts/information_theory/#information-bottleneck">Information Bottleneck</a></h2>
<ul>
<li>are solutions in bits smaller as the model size increases?
<ul>
<li>how would you measure the solution bit size?</li>
<li>intuitively the solution size actually should just increase as the transofmer gets bigger</li>
</ul>
</li>
<li>read
<ul>
<li>information probing on description length of solutions</li>
<li>do softmaxes become hardmaxes?</li>
</ul>
</li>
<li>people
<ul>
<li><a href="https://www.cs.washington.edu/people/faculty/lsz">https://www.cs.washington.edu/people/faculty/lsz</a></li>
</ul>
</li>
<li>three approaches
<ul>
<li>circuit complexity - establishing upper bounds on what this thing is</li>
<li>renormalization groups - is there something there? information bottleneck?</li>
<li>probing - probing the description length?</li>
</ul>
</li>
<li>motivate the paper by saying, &ldquo;we should want to predict what types of capabiltiies can emerge, and it&rsquo;s important to take lessons from statistical physics&rdquo;</li>
</ul>
<h3 id="on-the-information-bottleneck-theory-of-deep-learning">On the Information Bottleneck Theory of Deep Learning</h3>
<ul>
<li>Cites
<ul>
<li>Tishby and Zalavsky 2015,Shwartz-Ziv &amp; Tishby, 2017</li>
<li>Tishby 1999, information bottleneck</li>
<li>deep nn complex non-linear learning trajectories (Saxe et al 2014)</li>
<li>baldi and hornik, optimization problems remain non-convex (1989)</li>
</ul>
</li>
<li>Deep learning as representation learners</li>
<li>information bottleneck deep learning, 3 main claims
<ul>
<li>deep nns undergo two distcint phases, an initial fitting phase and a compression one, where fitting is when the mutual information increases, compression is where mutual information decreases</li>
<li>unclear b/c tanh exhibits a compression, relu does not</li>
<li>networks that do not do compression exhibit generalization</li>
<li>SIDEBAR: does this mean that mesa optimizers require compression?</li>
</ul>
</li>
<li>learning trajectories are not easily predictable, mesa optimizers may arise</li>
<li>SGD - two phases from shwartz-ziv and tishby, 2017, distinguishing between drift phase (mean of gradients over training samples) and diffusion phase (mean becomes smaller than the standard devianation of the gradients)</li>
</ul>
<h3 id="paper-questions">Paper questions</h3>
<ul>
<li>does information probing, circuit complexity, or information theory help us get anywhere?</li>
</ul>
<h3 id="effects-of-parameter-norm-growth-during-transformer-training">Effects of Parameter Norm Growth During Transformer Training</h3>
<ul>
<li>suggests that transformers learn due to inductive bias (aka projecting out the set of assumptions)</li>
<li>studies the growth in the l2 norm during training</li>
<li>norm growth occurs, then reaches a discretized network with saturated network activations
<ul>
<li>saturation - neuron predominantly outputs values close to the asymptotic ends of the bounded activation function
<ul>
<li>softmaxes become hardmaxes - <a href="https://ieeexplore.ieee.org/document/7376778">https://ieeexplore.ieee.org/document/7376778</a></li>
<li>measuring saturation in neural networks - <a href="https://ieeexplore.ieee.org/document/7376778">https://ieeexplore.ieee.org/document/7376778</a></li>
</ul>
</li>
<li>the l2 norm grows, aka norm growth</li>
<li>previous work on feedforward networks
<ul>
<li>li and auorara 2019, ji and telgarsky 2020</li>
</ul>
</li>
<li>anywhere the norm diverges during the training approaches a saturated network</li>
</ul>
</li>
<li>saturation allows for approximation via circuit complexity</li>
<li>transformers can implement a countering mechanism (Bhattmishra et al 2020)
<ul>
<li>Bhattmishra also finds that trained networks learn to recongize counter languages that rely on computing means</li>
</ul>
</li>
</ul>
<h2 id="discussions">Discussions</h2>
<h3 id="reddit">reddit</h3>
<ul>
<li><a href="https://old.reddit.com/r/MachineLearning/comments/em3ynp/d_trying_to_wrap_my_head_around_the_information/">https://old.reddit.com/r/MachineLearning/comments/em3ynp/d_trying_to_wrap_my_head_around_the_information/</a></li>
<li><a href="https://old.reddit.com/r/MachineLearning/comments/elmgsz/r_on_the_information_bottleneck_theory_of_deep/">https://old.reddit.com/r/MachineLearning/comments/elmgsz/r_on_the_information_bottleneck_theory_of_deep/</a></li>
</ul>
<h3 id="other-papers">Other papers</h3>
<ul>
<li>survey - <a href="https://arxiv.org/pdf/1904.03743.pdf">https://arxiv.org/pdf/1904.03743.pdf</a></li>
<li>similar paper that tries to measure model complexity with curve activation functions <a href="https://arxiv.org/abs/2006.08962">https://arxiv.org/abs/2006.08962</a></li>
<li><a href="https://arxiv.org/pdf/1909.11396.pdf">https://arxiv.org/pdf/1909.11396.pdf</a></li>
<li>investigating dropout regularization and model complexity in NN: <a href="https://arxiv.org/abs/2108.06628">https://arxiv.org/abs/2108.06628</a></li>
<li><a href="https://arxiv.org/pdf/1909.11396.pdf">https://arxiv.org/pdf/1909.11396.pdf</a></li>
</ul>
<h2 id="videos">Videos</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=bLqJHjXihK8&amp;t=960s">https://www.youtube.com/watch?v=bLqJHjXihK8&amp;t=960s</a></li>
</ul>
<h2 id="the-information-bottleneck-problem-and-its-applications-in-machine-learning">The Information Bottleneck Problem and Its Applications in Machine Learning</h2>
<ul>
<li>survey - <a href="https://arxiv.org/pdf/2004.14941.pdf">https://arxiv.org/pdf/2004.14941.pdf</a></li>
</ul>
<h2 id="deep-variational-information-bottleneck">Deep variational information bottleneck</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1612.00410.pdf">https://arxiv.org/pdf/1612.00410.pdf</a></li>
</ul>
<h2 id="saxe-et-al-paper">Saxe et al Paper</h2>
<ul>
<li><a href="https://openreview.net/forum?id=ry_WPG-A">https://openreview.net/forum?id=ry_WPG-A</a></li>
<li><a href="https://old.reddit.com/r/MachineLearning/comments/79efus/r_on_the_information_bottleneck_theory_of_deep/">https://old.reddit.com/r/MachineLearning/comments/79efus/r_on_the_information_bottleneck_theory_of_deep/</a></li>
<li><a href="https://old.reddit.com/r/MachineLearning/comments/elmgsz/r_on_the_information_bottleneck_theory_of_deep/">https://old.reddit.com/r/MachineLearning/comments/elmgsz/r_on_the_information_bottleneck_theory_of_deep/</a></li>
</ul>
<h2 id="code">Code</h2>
<ul>
<li><a href="https://github.com/ravidziv/IDNNs">https://github.com/ravidziv/IDNNs</a> - code for 2015 paper</li>
</ul>
<h2 id="scalable-mutual-information-using-dependence-graphs">Scalable Mutual Information Using Dependence Graphs</h2>
<ul>
<li><a href="https://arxiv.org/abs/1801.09125">https://arxiv.org/abs/1801.09125</a></li>
<li>Claims to go against the saxe paper</li>
<li><a href="https://github.com/mrtnoshad/EDGE/blob/master/information_plane/main.py">https://github.com/mrtnoshad/EDGE/blob/master/information_plane/main.py</a> - code for 2018 paper on EDGE, the scalable linear measure of <a href="/posts/information_theory/#mutual-information">Mutual Information</a></li>
</ul>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/fall_2021_classes/">fall 2021 classes</a></li>
       
          <li><a href="/posts/information_theory/">information theory</a></li>
       
          <li><a href="/posts/mesa_optimizer_and_scaling_laws_draft/">mesa optimizer and scaling laws draft</a></li>
       
          <li><a href="/posts/scaling_laws_survey_paper_information_bottleneck_minimum_description_length_etc/">scaling laws survey paper (information bottleneck, minimum description length, etc)</a></li>
       
          <li><a href="/posts/the_information_plane_within_deep_learning/">the information plane within deep learning</a></li>
       
          <li><a href="/posts/20211031204616-truthfulqa/">truthfulqa</a></li>
       
          <li><a href="/posts/weng_are_deep_neural_networks_dramatically_overfitted/">Weng: Are Deep Neural Networks Dramatically Overfitted?</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>
</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
