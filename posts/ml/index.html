<!DOCTYPE html>
<html><title>ml</title>


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />


<link rel="stylesheet" href="/css/main.min.532078bb650bec3433e8e9db19e0453faa66c9cc0ce761054558a91d87039e9d.css"/>
<script defer src="/en.search.min.c24d2b2f5158135525a6ca1547b575cecfb775550a92e8dedec8686bd221e5cb.js" integrity="sha256-wk0rL1FYE1UlpsoVR7V1zs&#43;3dVUKkuje3shoa9Ih5cs="></script>

<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<body><header>
    <a href="/" id="logo">
        <img src="/svg/icon.svg" alt="icon" id="raccoon" />

    </a>
    <h3 class="site-title">عجفت الغور</h3>
    <div id="search">
        
<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>

    </div>
</header>

<div class="grid-container">
  <div class="grid">
    <div class="page" data-level="1">
      <div class="content">
          <h1>ml</h1>

          <p>Tags: <a href="/posts/computers/">Computers</a>, <a href="/posts/math/">math</a></p>
<h2 id="failures">Failures</h2>
<ul>
<li>Zillow winds down their ml model house flipping scheme: <a href="https://www.prnewswire.com/news-releases/zillow-group-reports-third-quarter-2021-financial-results--shares-plan-to-wind-down-zillow-offers-operations-301414460.html?tc=eml_cleartime">https://www.prnewswire.com/news-releases/zillow-group-reports-third-quarter-2021-financial-results--shares-plan-to-wind-down-zillow-offers-operations-301414460.html?tc=eml_cleartime</a></li>
</ul>
<h2 id="ml-tools">ML Tools</h2>
<ul>
<li>visualizer: <a href="https://github.com/lutzroeder/netron">https://github.com/lutzroeder/netron</a></li>
<li>voice coloning: <a href="https://git.ecker.tech/mrq/ai-voice-cloning/wiki/Installation">https://git.ecker.tech/mrq/ai-voice-cloning/wiki/Installation</a></li>
<li><a href="https://seamless.metademolab.com/expressive/?utm_source=metaai&amp;utm_medium=web&amp;utm_campaign=seamless&amp;utm_content=landing_page">https://seamless.metademolab.com/expressive/?utm_source=metaai&amp;utm_medium=web&amp;utm_campaign=seamless&amp;utm_content=landing_page</a>
<ul>
<li>seamless voice models from FB</li>
</ul>
</li>
</ul>
<h2 id="tree-based-models">Tree Based Models</h2>
<ul>
<li><a href="https://arxiv.org/abs/2207.08815">https://arxiv.org/abs/2207.08815</a></li>
</ul>
<h2 id="post-training-methods">Post Training Methods</h2>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Method</th>
<th>Who supplies preferences?</th>
<th>Reward model?</th>
<th>Optimizer / loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>RLHF</td>
<td>Humans</td>
<td>Yes</td>
<td>RL (policy‑gradient) with KL to reference policy</td>
</tr>
<tr>
<td>RLAIF</td>
<td>LLM (AI)</td>
<td>Yes</td>
<td>RL (policy‑gradient) with KL to reference policy</td>
</tr>
<tr>
<td>d‑RLAIF</td>
<td>LLM (AI), on‑policy scoring</td>
<td>No — LLM provides scalar reward</td>
<td>RL (policy‑gradient) with KL to reference policy</td>
</tr>
<tr>
<td>DPO</td>
<td>Humans or AI (pairwise prefs)</td>
<td>No</td>
<td>Direct Preference Optimization (classification‑style)</td>
</tr>
</tbody>
</table>
<h2 id="dpo--direct-preference-optimization">DPO (direct preference optimization)</h2>
<ul>
<li>
<p>Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. “Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.” arXiv:2305.18290. Preprint, arXiv, July 29, 2024. <a href="https://doi.org/10.48550/arXiv.2305.18290">https://doi.org/10.48550/arXiv.2305.18290</a>. - <a href="/posts/papers/">papers</a></p>
</li>
<li>
<p>Used generally now, llama 4 was done with a combination of online RL (where we have ground truths) and then DPO</p>
</li>
<li>
<p>DPO saves you the effort of having to do any specific RM training</p>
</li>
<li>
<p>Core claim: the standard RLHF objective (maximize learned reward under a KL constraint to a reference policy) can be optimized <strong><strong>exactly</strong></strong> with a simple <strong><strong>binary cross-entropy</strong></strong> on preference pairs—no actor–critic loop, no on-policy sampling, minimal tuning. (Fig. 1, p.2)</p>
</li>
<li>
<p>Reparameterization (the “LM is a reward model” insight):</p>
<ul>
<li>Optimal policy under KL control: $ π^*(y \mid x) ∝ π<!-- raw HTML omitted -->\text{ref}<!-- raw HTML omitted -->(y \mid x)exp\!\left(\tfrac{1}{\beta} r(x,y)\right) $.</li>
<li>Invert to express reward via policy: $ r(x,y) = β log \tfrac{\pi^\star(y \mid x)}{π<!-- raw HTML omitted -->\text{ref}<!-- raw HTML omitted -->(y \mid x)} + β log Z(x) $.</li>
<li>Bradley–Terry preference prob becomes a function of <strong><strong>policy log-ratio</strong></strong> only; the partition cancels. Resulting DPO loss:
\[
L_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}})= -\mathbb{E}_{(x,y_w,y_l)\sim D}\log\sigma\!\Big(\beta\big[\log\tfrac{\pi_\theta(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)}-\log\tfrac{\pi_\theta(y_l\mid x)}{\pi_{\text{ref}}(y_l\mid x)}\big]\Big).
\]</li>
</ul>
</li>
<li>
<p>Why this avoids degeneration: the gradient <strong><strong>upweights</strong></strong> examples where the current implicit reward \(\hat r_\theta(x,y)=\beta\log\tfrac{\pi_\theta}{\pi_{\text{ref}}}\) ranks the loser above the winner:
\[
\nabla_\theta L_{\text{DPO}} = -\beta\,\mathbb{E}\!\left[\sigma\!\left(\hat r_\theta(x,y_l)-\hat r_\theta(x,y_w)\right)\,\big(\nabla\log\pi(y_w\!\mid\!x)-\nabla\log\pi(y_l\!\mid\!x)\big)\right].
\]
The per-example weight curbs the naive “push ratios” failure mode observed with unweighted objectives. (pp.4–5)</p>
</li>
<li>
<p>Theoretical framing:</p>
<ul>
<li>Rewards differing by any \(f(x)\) are <strong><strong>equivalent</strong></strong> for both preference likelihood and the induced KL-constrained optimal policy. DPO picks the unique representative in each equivalence class with $ ∑_y π<!-- raw HTML omitted -->\text{ref}<!-- raw HTML omitted -->(y\mid x)exp(\tfrac{1}{\beta}r(x,y))=1 $, i.e., one whose optimal policy is directly \(\pi_\theta\). (pp.5–6)</li>
<li>Actor–critic diagnosis: PPO needs a baseline/“soft value” (normalizer) for stability; DPO’s reparameterization <strong><strong>bakes in</strong></strong> that normalization, avoiding variance/baseline hacks. (p.6)</li>
</ul>
</li>
<li>
<p>Practical recipe:</p>
<ul>
<li>Data: \((x,y_w,y_l)\) from human (or AI) preferences.</li>
<li>Reference: use \(\pi_{\text{SFT}}\) if available; otherwise fit \(\pi_{\text{ref}}\) by MLE on \(y_w\) only to reduce ref–data shift. (p.5)</li>
<li>One loss, no rollouts, no separate RM training, no KL penalties at sequence time—KL pressure is implicit via the log-ratios.</li>
</ul>
</li>
<li>
<p>Empirical signals (paper-scale models up to ~6B):</p>
<ul>
<li><strong><strong>Reward–KL frontier</strong></strong> (IMDb sentiment): DPO strictly dominates PPO (even PPO with ground-truth reward). (Fig. 2 left, p.7)</li>
<li><strong><strong>Summarization (Reddit TL;DR):</strong></strong> DPO ≈ <strong><strong>61%</strong></strong> GPT‑4 win rate vs ref at temp 0.0 vs PPO ≈ <strong><strong>57%</strong></strong> at its best temp; more robust to sampling temperature. (Fig. 2 right, p.7; p.9)</li>
<li><strong><strong>Dialogue (Anthropic HH, 1‑turn):</strong></strong> DPO is the only efficient method that <strong><strong>beats the dataset “chosen” responses</strong></strong>; roughly matches or exceeds “best‑of‑128” sampling without its test‑time cost. (Fig. 3, p.8; p.9)</li>
<li><strong><strong>OOD generalization (CNN/DailyMail):</strong></strong> DPO &gt; PPO on GPT‑4 win rate when evaluated off‑distribution. (Table 1, p.9)</li>
<li><strong><strong>Best‑of‑N plateau:</strong></strong> gains saturate around N≈64–128; DPO rivals this without expensive sampling. (Fig. 4, p.23)</li>
</ul>
</li>
<li>
<p>Hyperparams the authors actually used (sanity defaults, not sacred):</p>
<ul>
<li>\(\beta\) often \(0.1\); TL;DR used \(\beta=0.5\).</li>
<li>Batch 64; RMSProp; LR \(1\mathrm{e}{-6}\) with 150‑step linear warmup. (App. B, p.20)</li>
</ul>
</li>
<li>
<p>Caveats / limits worth remembering:</p>
<ul>
<li>OOD and self‑labeling dynamics need more study; reward over‑optimization can still bite (small late‑training dips). Scaling beyond 6B left for future work. (p.10)</li>
<li>Unlikelihood training is unstable on open‑ended tasks (degenerate text). (App. C/D; Table on p.22)</li>
</ul>
</li>
<li>
<p>TL;DR of the TL;DR: Train one model; treat \(\log\frac{\pi_\theta}{\pi_{\text{ref}}}\) as the implicit reward; fit it with BCE on pairwise prefs. You get PPO‑level (often better) alignment without PPO’s brittleness or cost. (Fig. 1–3, pp.2,7–8)</p>
</li>
</ul>
<h2 id="rl">RL</h2>
<ul>
<li>Fisher information matrix?</li>
</ul>
<h2 id="computer-vision">Computer Vision</h2>

          




   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   
      
   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
      
   

   

   

   

   



  <div class="bl-section">
    <h4>Links to this note</h4>
    <div class="backlinks">
        <ul>
       
          <li><a href="/posts/adam_optimizer/">adam optimizer</a></li>
       
          <li><a href="/posts/annotations/">annotations</a></li>
       
          <li><a href="/posts/approximate_nearest_neighbor/">approximate nearest neighbor</a></li>
       
          <li><a href="/posts/atomic/">atomic</a></li>
       
          <li><a href="/posts/automatic_differentiation/">automatic differentiation</a></li>
       
          <li><a href="/posts/backprop/">backprop</a></li>
       
          <li><a href="/posts/chatgpt/">chatgpt</a></li>
       
          <li><a href="/posts/clustering/">clustering</a></li>
       
          <li><a href="/posts/covolutional_neural_networks/">convolutional neural networks (cnn)</a></li>
       
          <li><a href="/posts/convolutions/">convolutions</a></li>
       
          <li><a href="/posts/cyc/">cyc</a></li>
       
          <li><a href="/posts/datasets/">datasets</a></li>
       
          <li><a href="/posts/deep_learning/">deep learning</a></li>
       
          <li><a href="/posts/embedding_text_into_vector_spaces/">embedding text into vector spaces</a></li>
       
          <li><a href="/posts/energy_based_models/">energy based models</a></li>
       
          <li><a href="/posts/ensemble_learning/">ensemble learning</a></li>
       
          <li><a href="/posts/esim_model/">ESIM model</a></li>
       
          <li><a href="/posts/explaining_away/">explaining away</a></li>
       
          <li><a href="/posts/fact_extraction_and_verification_2018/">Fact Extraction and Verification (2018)</a></li>
       
          <li><a href="/posts/fasttext/">fasttext</a></li>
       
          <li><a href="/posts/federenko_emnlp_keynote/">federenko - EMNLP Keynote</a></li>
       
          <li><a href="/posts/glue/">glue</a></li>
       
          <li><a href="/posts/goode_artifical_intelligence_and_the_future_of_nationalism/">Goode: Artifical Intelligence and the Future of Nationalism</a></li>
       
          <li><a href="/posts/gpt2/">gpt2</a></li>
       
          <li><a href="/posts/gradient_descent/">gradient descent</a></li>
       
          <li><a href="/posts/hierarchical_navigable_small_world_graph/">hierarchical navigable small-world graph (HNSW)</a></li>
       
          <li><a href="/posts/higher_fb_library/">higher (fb library)</a></li>
       
          <li><a href="/posts/jupyter_notebooks/">jupyter notebooks</a></li>
       
          <li><a href="/posts/kubeflow/">kubeflow</a></li>
       
          <li><a href="/posts/llms/">llms</a></li>
       
          <li><a href="/posts/loss_function/">loss function</a></li>
       
          <li><a href="/posts/misinterpretations_of_what_ai_is/">misinterpretations of what ai is</a></li>
       
          <li><a href="/posts/ml_conferences/">ml conferences</a></li>
       
          <li><a href="/posts/multi_level_ir/">multi-level IR</a></li>
       
          <li><a href="/posts/multirc/">MultiRC</a></li>
       
          <li><a href="/posts/nearest_neighbor/">nearest neighbor</a></li>
       
          <li><a href="/posts/nlp/">nlp</a></li>
       
          <li><a href="/posts/openbook/">OpenBook</a></li>
       
          <li><a href="/posts/optuna/">optuna</a></li>
       
          <li><a href="/posts/pandas/">pandas</a></li>
       
          <li><a href="/posts/positive_labeling/">positive labeling</a></li>
       
          <li><a href="/posts/product_quantization/">product quantization</a></li>
       
          <li><a href="/posts/pytorch/">pytorch</a></li>
       
          <li><a href="/posts/recurrent_neural_networks/">recurrent neural networks (rnn)</a></li>
       
          <li><a href="/posts/relu/">relu</a></li>
       
          <li><a href="/posts/sacred/">sacred</a></li>
       
          <li><a href="/posts/softmax/">softmax</a></li>
       
          <li><a href="/posts/structured_space_modeling/">structured space modeling</a></li>
       
          <li><a href="/posts/superintelligence_as_an_infohazard/">superintelligence as an infohazard</a></li>
       
          <li><a href="/posts/sutton_the_bitter_lesson/">Sutton: The Bitter Lesson</a></li>
       
          <li><a href="/posts/tensorboard/">tensorboard</a></li>
       
          <li><a href="/posts/tensorflow/">tensorflow</a></li>
       
          <li><a href="/posts/training_validation_and_tests_sets/">training, validation, and tests sets</a></li>
       
          <li><a href="/posts/visual_question_answering/">visual question answering</a></li>
       
          <li><a href="/posts/winograd_schemas/">winograd schemas</a></li>
       
          <li><a href="/posts/zhang_et_al_tropical_geometry_of_deep_neural_networks/">Zhang et al: Tropical Geometry of Deep Neural Networks</a></li>
       
     </ul>
    </div>
  </div>


      </div>
    </div>
  </div>
</div>

<script src="/js/URI.js" type="text/javascript"></script>

<script src="/js/page.js" type="text/javascript"></script>
</body>
</html>
